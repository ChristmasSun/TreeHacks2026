<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Tutor</title>
  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: #000;
      color: white;
      height: 100vh;
      display: flex;
      flex-direction: column;
    }
    .header {
      padding: 12px 16px;
      background: rgba(0,0,0,0.8);
      display: flex;
      justify-content: space-between;
      align-items: center;
      border-bottom: 1px solid rgba(255,255,255,0.1);
    }
    .header h1 { font-size: 14px; font-weight: 500; opacity: 0.8; }
    .close-btn {
      background: none;
      border: none;
      color: white;
      opacity: 0.6;
      cursor: pointer;
      font-size: 20px;
      padding: 4px 8px;
    }
    .close-btn:hover { opacity: 1; }
    .main {
      flex: 1;
      display: flex;
      gap: 16px;
      padding: 16px;
      overflow: hidden;
    }
    .video-container {
      flex: 1;
      background: #111;
      border-radius: 12px;
      overflow: hidden;
      position: relative;
      min-height: 300px;
    }
    #avatar-video {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    .loading, .error {
      position: absolute;
      inset: 0;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      background: #111;
    }
    .spinner {
      width: 40px;
      height: 40px;
      border: 3px solid rgba(59, 130, 246, 0.3);
      border-top-color: #3b82f6;
      border-radius: 50%;
      animation: spin 1s linear infinite;
    }
    @keyframes spin { to { transform: rotate(360deg); } }
    .loading p, .error p { margin-top: 12px; opacity: 0.7; font-size: 14px; }
    .error { color: #f87171; }
    .retry-btn {
      margin-top: 16px;
      padding: 8px 16px;
      background: #3b82f6;
      border: none;
      border-radius: 8px;
      color: white;
      cursor: pointer;
    }
    .chat {
      width: 300px;
      display: flex;
      flex-direction: column;
      background: rgba(255,255,255,0.05);
      border-radius: 12px;
      overflow: hidden;
    }
    .messages {
      flex: 1;
      overflow-y: auto;
      padding: 12px;
    }
    .message {
      margin-bottom: 8px;
      padding: 8px 12px;
      border-radius: 12px;
      max-width: 85%;
      font-size: 14px;
    }
    .message.student {
      background: #3b82f6;
      margin-left: auto;
    }
    .message.avatar {
      background: #374151;
    }
    .input-area {
      padding: 12px;
      border-top: 1px solid rgba(255,255,255,0.1);
      display: flex;
      gap: 8px;
    }
    .input-area input {
      flex: 1;
      background: #1f2937;
      border: 1px solid rgba(255,255,255,0.1);
      border-radius: 8px;
      padding: 8px 12px;
      color: white;
      font-size: 14px;
    }
    .input-area input:focus { outline: none; border-color: #3b82f6; }
    .input-area button {
      padding: 8px 16px;
      background: #3b82f6;
      border: none;
      border-radius: 8px;
      color: white;
      cursor: pointer;
    }
    .input-area button:disabled { background: #374151; cursor: not-allowed; }
    .mic-btn {
      padding: 8px 12px;
      background: #374151;
      border: none;
      border-radius: 8px;
      color: white;
      cursor: pointer;
      font-size: 18px;
      transition: background 0.2s;
    }
    .mic-btn:hover { background: #4b5563; }
    .mic-btn.recording { background: #ef4444; animation: pulse-mic 1s infinite; }
    .mic-btn:disabled { opacity: 0.5; cursor: not-allowed; }
    @keyframes pulse-mic {
      0%, 100% { box-shadow: 0 0 0 0 rgba(239, 68, 68, 0.5); }
      50% { box-shadow: 0 0 0 8px rgba(239, 68, 68, 0); }
    }
    .hidden { display: none !important; }
    .mode-toggle {
      display: flex;
      gap: 4px;
      padding: 4px;
      background: rgba(255,255,255,0.1);
      border-radius: 8px;
      margin-bottom: 12px;
    }
    .mode-btn {
      flex: 1;
      padding: 8px 12px;
      border: none;
      border-radius: 6px;
      background: transparent;
      color: rgba(255,255,255,0.6);
      cursor: pointer;
      font-size: 12px;
      transition: all 0.2s;
    }
    .mode-btn.active {
      background: #3b82f6;
      color: white;
    }
    .mode-btn:hover:not(.active) { background: rgba(255,255,255,0.1); }
    .speaking-indicator {
      position: absolute;
      bottom: 12px;
      left: 12px;
      background: rgba(0,0,0,0.6);
      padding: 6px 12px;
      border-radius: 20px;
      font-size: 12px;
      display: flex;
      align-items: center;
      gap: 8px;
    }
    .speaking-dots {
      display: flex;
      gap: 3px;
    }
    .speaking-dots span {
      width: 6px;
      height: 6px;
      background: #3b82f6;
      border-radius: 50%;
      animation: pulse 1s infinite;
    }
    .speaking-dots span:nth-child(2) { animation-delay: 0.1s; }
    .speaking-dots span:nth-child(3) { animation-delay: 0.2s; }
    @keyframes pulse {
      0%, 100% { opacity: 0.4; transform: scale(0.8); }
      50% { opacity: 1; transform: scale(1); }
    }
    /* Explainer video overlay */
    .explainer-overlay {
      position: absolute;
      inset: 0;
      background: rgba(0,0,0,0.95);
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      z-index: 100;
    }
    .explainer-overlay.hidden { display: none; }
    .explainer-header {
      padding: 12px 20px;
      text-align: center;
    }
    .explainer-header h2 {
      font-size: 18px;
      font-weight: 600;
      margin-bottom: 4px;
    }
    .explainer-header p {
      font-size: 14px;
      opacity: 0.7;
    }
    #explainer-video {
      max-width: 100%;
      max-height: calc(100% - 100px);
      border-radius: 8px;
    }
    .explainer-controls {
      padding: 12px 20px;
      display: flex;
      gap: 12px;
    }
    .explainer-btn {
      padding: 8px 20px;
      border: none;
      border-radius: 8px;
      cursor: pointer;
      font-size: 14px;
      transition: all 0.2s;
    }
    .explainer-btn.primary {
      background: #3b82f6;
      color: white;
    }
    .explainer-btn.primary:hover { background: #2563eb; }
    .explainer-btn.secondary {
      background: #374151;
      color: white;
    }
    .explainer-btn.secondary:hover { background: #4b5563; }
  </style>
</head>
<body>
  <div class="header">
    <h1>AI Tutor &bull; <span id="student-name">Student</span></h1>
    <button class="close-btn" onclick="closeWindow()">&times;</button>
  </div>
  
  <div class="main">
    <div class="video-container">
      <div class="loading" id="loading">
        <div class="spinner"></div>
        <p>Starting AI Tutor...</p>
      </div>
      <div class="error hidden" id="error">
        <p>Connection Error</p>
        <p id="error-msg"></p>
        <button class="retry-btn" onclick="startAvatar()">Retry</button>
      </div>
      <video id="avatar-video" autoplay playsinline></video>
      <!-- Explainer video overlay for quiz -->
      <div class="explainer-overlay hidden" id="explainer-overlay">
        <div class="explainer-header">
          <h2 id="explainer-title">Explainer Video</h2>
          <p id="explainer-concept">Watch to learn more about this concept</p>
        </div>
        <video id="explainer-video" controls></video>
        <div class="explainer-controls">
          <button class="explainer-btn secondary" onclick="skipExplainer()">Skip</button>
          <button class="explainer-btn primary hidden" id="continue-btn" onclick="continueAfterExplainer()">Continue Quiz</button>
        </div>
      </div>
      <div class="speaking-indicator hidden" id="speaking">
        <div class="speaking-dots">
          <span></span><span></span><span></span>
        </div>
        <span>Speaking...</span>
      </div>
    </div>
    
    <div class="chat">
      <div class="mode-toggle">
        <div id="voice-status" class="mode-btn active" style="flex:1; cursor:default; text-align:center;">
          üé§ Always Listening
        </div>
      </div>
      <div class="messages" id="messages"></div>
      <div class="input-area" id="tutor-input">
        <button id="mic-btn" class="mic-btn recording" onclick="toggleMic()" title="Mute/Unmute">üé§</button>
        <input type="text" id="input" placeholder="üé§ Listening..." disabled>
        <button id="send-btn" onclick="sendMessage()" disabled>Send</button>
      </div>
    </div>
  </div>

  <script type="module">
    import { Room, RoomEvent } from 'https://esm.sh/livekit-client@latest';

    let livekitRoom = null;
    let sessionId = null;
    let heygenWsUrl = null;   // Passed to backend for LITE audio control
    let sessionToken = null;  // Session token for LITE WebSocket auth + session stop
    let conversationHistory = [];

    const params = new URLSearchParams(window.location.search);
    const studentName = params.get('name') || 'Student';
    const backendUrl = params.get('backend') || 'http://127.0.0.1:8000';

    document.getElementById('student-name').textContent = studentName;

    window.closeWindow = function() {
      if (livekitRoom) livekitRoom.disconnect().catch(() => {});
      window.close();
    }

    function showLoading() {
      document.getElementById('loading').classList.remove('hidden');
      document.getElementById('error').classList.add('hidden');
    }

    function showError(msg) {
      document.getElementById('loading').classList.add('hidden');
      document.getElementById('error').classList.remove('hidden');
      document.getElementById('error-msg').textContent = msg;
    }

    function hideOverlays() {
      document.getElementById('loading').classList.add('hidden');
      document.getElementById('error').classList.add('hidden');
    }

    function addMessage(role, text) {
      const div = document.createElement('div');
      div.className = 'message ' + role;
      div.textContent = text;
      document.getElementById('messages').appendChild(div);
      document.getElementById('messages').scrollTop = 9999;
    }

    window.startAvatar = async function() {
      showLoading();

      try {
        // 1. Create LITE session via backend
        console.log('Fetching HeyGen LITE session...');
        const res = await fetch(backendUrl + '/api/heygen-token');
        if (!res.ok) {
          const errText = await res.text();
          throw new Error('HeyGen session failed: ' + errText);
        }
        const sessionData = await res.json();
        console.log('Session data:', JSON.stringify(sessionData, null, 2));

        sessionId = sessionData.session_id;
        heygenWsUrl = sessionData.ws_url;
        sessionToken = sessionData.session_token;
        const livekitUrl = sessionData.livekit_url;
        const accessToken = sessionData.livekit_token;

        console.log('Got LITE session:', sessionId, 'ws_url:', heygenWsUrl ? 'yes' : 'MISSING', 'livekit:', livekitUrl ? 'yes' : 'MISSING');

        // Start audio pipeline immediately (don't wait for video)
        // Kill any stale reconnect loops, then start fresh
        shouldReconnect = false;
        stopAudioPipeline();

        const greeting = 'Hello ' + studentName + '! I\'m your AI Professor. I have context from the lecture, so just ask me anything!';
        addMessage('avatar', greeting);
        document.getElementById('input').disabled = false;
        document.getElementById('send-btn').disabled = false;
        document.getElementById('mic-btn').disabled = false;

        shouldReconnect = true;
        startAudioPipeline();

        if (!livekitUrl || !accessToken) {
          console.warn('Missing LiveKit credentials ‚Äî video will not load, but audio still works');
          return;
        }

        // 2. Connect to LiveKit for video rendering
        livekitRoom = new Room({
          adaptiveStream: true,
          dynacast: true,
        });

        livekitRoom.on(RoomEvent.TrackSubscribed, (track, publication, participant) => {
          console.log('Track subscribed:', track.kind, participant.identity);
          const video = document.getElementById('avatar-video');

          if (track.kind === 'video') {
            track.attach(video);
            video.muted = false;
            video.volume = 1.0;
            hideOverlays();
          }

          if (track.kind === 'audio') {
            track.attach(video);
          }
        });

        livekitRoom.on(RoomEvent.Disconnected, () => {
          console.log('LiveKit disconnected');
        });

        await livekitRoom.connect(livekitUrl, accessToken);
        console.log('Connected to LiveKit room');

        // Check for existing tracks (in case they arrived before our listener)
        livekitRoom.remoteParticipants.forEach((participant) => {
          participant.trackPublications.forEach((publication) => {
            if (publication.track) {
              const video = document.getElementById('avatar-video');
              if (publication.track.kind === 'video') {
                publication.track.attach(video);
                hideOverlays();
              }
              if (publication.track.kind === 'audio') {
                publication.track.attach(video);
              }
            }
          });
        });

      } catch (err) {
        console.error('Avatar start error:', err);
        showError(err.message || 'Failed to connect');
      }
    }

    window.sendMessage = async function() {
      const input = document.getElementById('input');
      const text = input.value.trim();
      if (!text) return;

      input.value = '';
      addMessage('student', text);
      conversationHistory.push({ role: 'student', text });

      try {
        const llmRes = await fetch(backendUrl + '/api/tutor-response', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            message: text,
            student_name: studentName,
            history: conversationHistory
          })
        });

        let response;
        if (llmRes.ok) {
          const data = await llmRes.json();
          response = data.response;
        } else {
          response = 'Sorry, I had trouble processing that. Could you try again?';
        }

        addMessage('avatar', response);
        conversationHistory.push({ role: 'avatar', text: response });
        // TTS + lip-sync handled by backend via LITE mode (no avatarSpeak needed)
      } catch (err) {
        console.error('Send error:', err);
      }
    }

    document.getElementById('input').addEventListener('keypress', (e) => {
      if (e.key === 'Enter') window.sendMessage();
    });

    // ============ VOICE PIPELINE (AudioWorklet ‚Üí Backend WebSocket ‚Üí Silero VAD + Deepgram nova-3) ============
    let isListening = false;
    let isSpeakingResponse = false;
    let isMuted = false;
    let shouldReconnect = false;  // Only reconnect after startAvatar succeeds
    let audioWs = null;
    let audioContext = null;
    let audioWorkletNode = null;
    let mediaStream = null;

    function updateVoiceStatus(status) {
      const statusEl = document.getElementById('voice-status');
      const micBtn = document.getElementById('mic-btn');

      switch(status) {
        case 'listening':
          statusEl.textContent = 'Listening...';
          statusEl.style.background = '#10b981';
          micBtn.classList.add('recording');
          break;
        case 'thinking':
          statusEl.textContent = 'Thinking...';
          statusEl.style.background = '#f59e0b';
          micBtn.classList.remove('recording');
          break;
        case 'speaking':
          statusEl.textContent = 'Speaking (interrupt anytime)';
          statusEl.style.background = '#3b82f6';
          micBtn.classList.add('recording');
          break;
        case 'muted':
          statusEl.textContent = 'Muted';
          statusEl.style.background = '#6b7280';
          micBtn.classList.remove('recording');
          break;
      }
    }

    // AudioWorklet processor with resampling support (data URI for Electron compat)
    const pcmProcessorCode = `
      class PCMProcessor extends AudioWorkletProcessor {
        constructor(options) {
          super();
          // Target: 512 samples at 16kHz (32ms chunks)
          this.targetRate = 16000;
          this.sourceRate = options.processorOptions?.sampleRate || sampleRate;
          this.ratio = this.sourceRate / this.targetRate;
          this.buffer = new Float32Array(512);
          this.bufferIndex = 0;
          this.srcIndex = 0; // fractional source index for resampling
        }
        process(inputs) {
          const input = inputs[0]?.[0];
          if (!input) return true;

          if (Math.abs(this.ratio - 1.0) < 0.01) {
            // No resampling needed (source is already 16kHz)
            for (let i = 0; i < input.length; i++) {
              this.buffer[this.bufferIndex++] = input[i];
              if (this.bufferIndex === 512) {
                this.port.postMessage(this.buffer.slice());
                this.bufferIndex = 0;
              }
            }
          } else {
            // Linear interpolation downsampling
            for (let i = 0; i < input.length; i++) {
              this.srcIndex += 1;
              while (this.srcIndex >= this.ratio) {
                this.srcIndex -= this.ratio;
                // Simple nearest-neighbor for speed
                this.buffer[this.bufferIndex++] = input[i];
                if (this.bufferIndex === 512) {
                  this.port.postMessage(this.buffer.slice());
                  this.bufferIndex = 0;
                }
              }
            }
          }
          return true;
        }
      }
      registerProcessor('pcm-processor', PCMProcessor);
    `;
    // Use data URI ‚Äî Blob URLs can fail in file:// or Electron contexts
    const processorDataUrl = 'data:application/javascript;base64,' + btoa(pcmProcessorCode);

    async function startAudioPipeline() {
      // Kill any existing connection first
      if (audioWs) {
        const old = audioWs;
        audioWs = null;
        try { old.onclose = null; old.close(); } catch(e) {}
      }

      console.log('Starting audio pipeline... heygenWsUrl=' + (heygenWsUrl ? 'set' : 'MISSING'));

      try {
        // 1. Connect WebSocket to backend audio endpoint
        const wsUrl = backendUrl.replace('http', 'ws') + '/ws/audio';
        audioWs = new WebSocket(wsUrl);

        audioWs.onopen = () => {
          console.log('Audio WebSocket connected');
          audioWs.send(JSON.stringify({
            type: 'start_session',
            student_name: studentName,
            meeting_id: params.get('meeting_id'),
            heygen_ws_url: heygenWsUrl,    // Backend connects to LITE WebSocket
            session_token: sessionToken,    // Auth for LITE session
          }));
        };

        audioWs.onmessage = async (event) => {
          const data = JSON.parse(event.data);

          switch (data.type) {
            case 'interim_transcript':
              // Update input field with live transcription
              document.getElementById('input').value = data.text;
              break;

            case 'vad_speech_start':
              if (!isSpeakingResponse) {
                updateVoiceStatus('listening');
              }
              break;

            case 'vad_speech_end':
              // Backend processed: VAD end ‚Üí LLM response ‚Üí PocketTTS ‚Üí HeyGen LITE
              console.log('Got response:', data.response);

              addMessage('student', data.transcript);
              addMessage('avatar', data.response);
              conversationHistory.push({ role: 'student', text: data.transcript });
              conversationHistory.push({ role: 'avatar', text: data.response });
              document.getElementById('input').value = '';

              updateVoiceStatus('speaking');
              isSpeakingResponse = true;
              document.getElementById('speaking').classList.remove('hidden');
              // Backend handles TTS + HeyGen LITE audio ‚Äî no avatarSpeak() needed
              break;

            case 'avatar_done':
              // Backend signals avatar finished speaking (via HeyGen LITE speak_ended)
              console.log('Avatar done speaking');
              isSpeakingResponse = false;
              document.getElementById('speaking').classList.add('hidden');
              updateVoiceStatus('listening');
              break;

            case 'interrupt_detected':
              // Backend VAD detected speech during avatar playback
              console.log('Interrupt detected');
              isSpeakingResponse = false;
              document.getElementById('speaking').classList.add('hidden');
              updateVoiceStatus('listening');
              break;

            case 'error':
              console.error('Backend error:', data.message);
              break;
          }
        };

        audioWs.onerror = (e) => {
          console.error('Audio WebSocket error:', e);
        };

        audioWs.onclose = (e) => {
          console.log('Audio WebSocket closed:', e.code);
          isListening = false;
          if (shouldReconnect && !isMuted) {
            console.log('Reconnecting audio pipeline in 2s...');
            setTimeout(startAudioPipeline, 2000);
          }
        };

        // 2. Set up AudioWorklet for PCM capture (resample to 16kHz in worklet)
        audioContext = new AudioContext();
        console.log('AudioContext sample rate:', audioContext.sampleRate);
        mediaStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          }
        });

        const source = audioContext.createMediaStreamSource(mediaStream);
        await audioContext.audioWorklet.addModule(processorDataUrl);
        audioWorkletNode = new AudioWorkletNode(audioContext, 'pcm-processor', {
          processorOptions: { sampleRate: audioContext.sampleRate }
        });

        // Convert float32 chunks to int16 and send via WebSocket
        audioWorkletNode.port.onmessage = (event) => {
          if (audioWs?.readyState !== WebSocket.OPEN || isMuted) return;
          const float32 = event.data;
          const int16 = new Int16Array(float32.length);
          for (let i = 0; i < float32.length; i++) {
            int16[i] = Math.max(-32768, Math.min(32767, Math.round(float32[i] * 32767)));
          }
          audioWs.send(int16.buffer);
        };

        source.connect(audioWorkletNode);
        // Connect to a silent destination to keep the worklet running
        audioWorkletNode.connect(audioContext.destination);

        isListening = true;
        updateVoiceStatus('listening');
        console.log('Audio pipeline started (16kHz PCM -> backend)');

      } catch (e) {
        console.error('Failed to start audio pipeline:', e);
        updateVoiceStatus('muted');
      }
    }

    function stopAudioPipeline() {
      console.log('Stopping audio pipeline...');
      isListening = false;

      if (audioWorkletNode) {
        audioWorkletNode.disconnect();
        audioWorkletNode = null;
      }
      if (mediaStream) {
        mediaStream.getTracks().forEach(t => t.stop());
        mediaStream = null;
      }
      if (audioContext) {
        audioContext.close().catch(() => {});
        audioContext = null;
      }
      if (audioWs) {
        audioWs.onclose = null;  // Prevent reconnect from firing
        try { audioWs.close(); } catch(e) {}
      }
      audioWs = null;
    }

    // Aliases for backward compat with explainer video code
    const startListening = startAudioPipeline;
    const stopListening = stopAudioPipeline;

    // Toggle mic button (mute/unmute)
    window.toggleMic = function() {
      if (isListening && !isMuted) {
        isMuted = true;
        stopAudioPipeline();
        updateVoiceStatus('muted');
        document.getElementById('mic-btn').textContent = 'Muted';
      } else {
        isMuted = false;
        document.getElementById('mic-btn').textContent = 'Mic';
        startAudioPipeline();
      }
    }

    // Start avatar and auto-enable voice
    window.startAvatar();

    // ============ EXPLAINER VIDEO HANDLING ============
    let currentExplainerJid = null;
    const explainerOverlay = document.getElementById('explainer-overlay');
    const explainerVideo = document.getElementById('explainer-video');
    const explainerTitle = document.getElementById('explainer-title');
    const explainerConcept = document.getElementById('explainer-concept');
    const continueBtn = document.getElementById('continue-btn');

    // Listen for play-explainer-video message from Electron main process
    // Since this is loaded with contextIsolation: false, we can use window events
    window.addEventListener('message', (event) => {
      if (event.data && event.data.type === 'play-explainer-video') {
        playExplainerVideo(event.data.payload);
      }
    });

    // Also listen via electron IPC if available
    if (typeof require !== 'undefined') {
      try {
        const { ipcRenderer } = require('electron');
        ipcRenderer.on('play-explainer-video', (event, payload) => {
          playExplainerVideo(payload);
        });
      } catch (e) {
        console.log('IPC not available, using postMessage');
      }
    }

    function playExplainerVideo(payload) {
      console.log('üé¨ Playing explainer video:', payload);

      const { concept, video_url, video_path, student_jid } = payload;
      currentExplainerJid = student_jid;

      // Update UI
      explainerTitle.textContent = 'Explainer: ' + (concept || 'Concept');
      explainerConcept.textContent = 'Watch this video to understand the concept better';

      // Determine video URL
      let videoSrc = video_url;
      if (!videoSrc && video_path) {
        // Use local file path
        videoSrc = video_path.startsWith('/') ? 'file://' + video_path : backendUrl + video_path;
      }

      if (!videoSrc) {
        console.error('No video source provided');
        addMessage('avatar', "Sorry, the explainer video isn't available right now.");
        notifyVideoCompleted();
        return;
      }

      // Pause avatar audio
      const avatarVideoEl = document.getElementById('avatar-video');
      if (avatarVideoEl) avatarVideoEl.muted = true;

      // Stop listening while watching video
      stopListening();

      // Show overlay and play video
      explainerVideo.src = videoSrc;
      explainerOverlay.classList.remove('hidden');
      continueBtn.classList.add('hidden');

      explainerVideo.play().catch(e => {
        console.error('Failed to play video:', e);
        addMessage('avatar', "Couldn't play the video. Let's continue.");
        hideExplainerAndContinue();
      });

      // Show continue button when video ends
      explainerVideo.onended = () => {
        console.log('üé¨ Explainer video ended');
        continueBtn.classList.remove('hidden');
      };

      explainerVideo.onerror = () => {
        console.error('Video error');
        addMessage('avatar', "There was an issue with the video. Let's continue.");
        hideExplainerAndContinue();
      };
    }

    window.skipExplainer = function() {
      console.log('‚è≠Ô∏è Skipping explainer video');
      explainerVideo.pause();
      hideExplainerAndContinue();
    };

    window.continueAfterExplainer = function() {
      console.log('‚û°Ô∏è Continuing after explainer');
      hideExplainerAndContinue();
    };

    function hideExplainerAndContinue() {
      explainerOverlay.classList.add('hidden');
      explainerVideo.pause();
      explainerVideo.src = '';

      // Unmute avatar
      const avatarVideoEl = document.getElementById('avatar-video');
      if (avatarVideoEl) avatarVideoEl.muted = false;

      // Notify backend that video is done
      notifyVideoCompleted();

      // Resume listening
      setTimeout(startListening, 500);
    }

    async function notifyVideoCompleted() {
      if (!currentExplainerJid) return;

      try {
        const response = await fetch(backendUrl + '/api/quiz/video-completed', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ student_jid: currentExplainerJid })
        });

        if (response.ok) {
          console.log('‚úÖ Notified backend video completed');
        }
      } catch (e) {
        console.error('Failed to notify video completion:', e);
      }

      currentExplainerJid = null;
    }
  </script>
</body>
</html>
