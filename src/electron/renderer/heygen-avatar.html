<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Tutor</title>
  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: #000;
      color: white;
      height: 100vh;
      display: flex;
      flex-direction: column;
    }
    .header {
      padding: 12px 16px;
      background: rgba(0,0,0,0.8);
      display: flex;
      justify-content: space-between;
      align-items: center;
      border-bottom: 1px solid rgba(255,255,255,0.1);
    }
    .header h1 { font-size: 14px; font-weight: 500; opacity: 0.8; }
    .close-btn {
      background: none;
      border: none;
      color: white;
      opacity: 0.6;
      cursor: pointer;
      font-size: 20px;
      padding: 4px 8px;
    }
    .close-btn:hover { opacity: 1; }
    .main {
      flex: 1;
      display: flex;
      gap: 16px;
      padding: 16px;
      overflow: hidden;
    }
    .video-container {
      flex: 1;
      background: #111;
      border-radius: 12px;
      overflow: hidden;
      position: relative;
      min-height: 300px;
    }
    #avatar-video {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    .loading, .error {
      position: absolute;
      inset: 0;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      background: #111;
    }
    .spinner {
      width: 40px;
      height: 40px;
      border: 3px solid rgba(59, 130, 246, 0.3);
      border-top-color: #3b82f6;
      border-radius: 50%;
      animation: spin 1s linear infinite;
    }
    @keyframes spin { to { transform: rotate(360deg); } }
    .loading p, .error p { margin-top: 12px; opacity: 0.7; font-size: 14px; }
    .error { color: #f87171; }
    .retry-btn {
      margin-top: 16px;
      padding: 8px 16px;
      background: #3b82f6;
      border: none;
      border-radius: 8px;
      color: white;
      cursor: pointer;
    }
    .chat {
      width: 300px;
      display: flex;
      flex-direction: column;
      background: rgba(255,255,255,0.05);
      border-radius: 12px;
      overflow: hidden;
    }
    .messages {
      flex: 1;
      overflow-y: auto;
      padding: 12px;
    }
    .message {
      margin-bottom: 8px;
      padding: 8px 12px;
      border-radius: 12px;
      max-width: 85%;
      font-size: 14px;
    }
    .message.student {
      background: #3b82f6;
      margin-left: auto;
    }
    .message.avatar {
      background: #374151;
    }
    .input-area {
      padding: 12px;
      border-top: 1px solid rgba(255,255,255,0.1);
      display: flex;
      gap: 8px;
    }
    .input-area input {
      flex: 1;
      background: #1f2937;
      border: 1px solid rgba(255,255,255,0.1);
      border-radius: 8px;
      padding: 8px 12px;
      color: white;
      font-size: 14px;
    }
    .input-area input:focus { outline: none; border-color: #3b82f6; }
    .input-area button {
      padding: 8px 16px;
      background: #3b82f6;
      border: none;
      border-radius: 8px;
      color: white;
      cursor: pointer;
    }
    .input-area button:disabled { background: #374151; cursor: not-allowed; }
    .mic-btn {
      padding: 8px 12px;
      background: #374151;
      border: none;
      border-radius: 8px;
      color: white;
      cursor: pointer;
      font-size: 18px;
      transition: background 0.2s;
    }
    .mic-btn:hover { background: #4b5563; }
    .mic-btn.recording { background: #ef4444; animation: pulse-mic 1s infinite; }
    .mic-btn:disabled { opacity: 0.5; cursor: not-allowed; }
    @keyframes pulse-mic {
      0%, 100% { box-shadow: 0 0 0 0 rgba(239, 68, 68, 0.5); }
      50% { box-shadow: 0 0 0 8px rgba(239, 68, 68, 0); }
    }
    .hidden { display: none !important; }
    .mode-toggle {
      display: flex;
      gap: 4px;
      padding: 4px;
      background: rgba(255,255,255,0.1);
      border-radius: 8px;
      margin-bottom: 12px;
    }
    .mode-btn {
      flex: 1;
      padding: 8px 12px;
      border: none;
      border-radius: 6px;
      background: transparent;
      color: rgba(255,255,255,0.6);
      cursor: pointer;
      font-size: 12px;
      transition: all 0.2s;
    }
    .mode-btn.active {
      background: #3b82f6;
      color: white;
    }
    .mode-btn:hover:not(.active) { background: rgba(255,255,255,0.1); }
    .speaking-indicator {
      position: absolute;
      bottom: 12px;
      left: 12px;
      background: rgba(0,0,0,0.6);
      padding: 6px 12px;
      border-radius: 20px;
      font-size: 12px;
      display: flex;
      align-items: center;
      gap: 8px;
    }
    .speaking-dots {
      display: flex;
      gap: 3px;
    }
    .speaking-dots span {
      width: 6px;
      height: 6px;
      background: #3b82f6;
      border-radius: 50%;
      animation: pulse 1s infinite;
    }
    .speaking-dots span:nth-child(2) { animation-delay: 0.1s; }
    .speaking-dots span:nth-child(3) { animation-delay: 0.2s; }
    @keyframes pulse {
      0%, 100% { opacity: 0.4; transform: scale(0.8); }
      50% { opacity: 1; transform: scale(1); }
    }
    /* Explainer video overlay */
    .explainer-overlay {
      position: absolute;
      inset: 0;
      background: rgba(0,0,0,0.95);
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      z-index: 100;
    }
    .explainer-overlay.hidden { display: none; }
    .explainer-header {
      padding: 12px 20px;
      text-align: center;
    }
    .explainer-header h2 {
      font-size: 18px;
      font-weight: 600;
      margin-bottom: 4px;
    }
    .explainer-header p {
      font-size: 14px;
      opacity: 0.7;
    }
    #explainer-video {
      max-width: 100%;
      max-height: calc(100% - 100px);
      border-radius: 8px;
    }
    .explainer-controls {
      padding: 12px 20px;
      display: flex;
      gap: 12px;
    }
    .explainer-btn {
      padding: 8px 20px;
      border: none;
      border-radius: 8px;
      cursor: pointer;
      font-size: 14px;
      transition: all 0.2s;
    }
    .explainer-btn.primary {
      background: #3b82f6;
      color: white;
    }
    .explainer-btn.primary:hover { background: #2563eb; }
    .explainer-btn.secondary {
      background: #374151;
      color: white;
    }
    .explainer-btn.secondary:hover { background: #4b5563; }
  </style>
</head>
<body>
  <div class="header">
    <h1>AI Tutor &bull; <span id="student-name">Student</span></h1>
    <button class="close-btn" onclick="closeWindow()">&times;</button>
  </div>
  
  <div class="main">
    <div class="video-container">
      <div class="loading" id="loading">
        <div class="spinner"></div>
        <p>Starting AI Tutor...</p>
      </div>
      <div class="error hidden" id="error">
        <p>Connection Error</p>
        <p id="error-msg"></p>
        <button class="retry-btn" onclick="startAvatar()">Retry</button>
      </div>
      <video id="avatar-video" autoplay playsinline></video>
      <!-- Explainer video overlay for quiz -->
      <div class="explainer-overlay hidden" id="explainer-overlay">
        <div class="explainer-header">
          <h2 id="explainer-title">Explainer Video</h2>
          <p id="explainer-concept">Watch to learn more about this concept</p>
        </div>
        <video id="explainer-video" controls></video>
        <div class="explainer-controls">
          <button class="explainer-btn secondary" onclick="skipExplainer()">Skip</button>
          <button class="explainer-btn primary hidden" id="continue-btn" onclick="continueAfterExplainer()">Continue Quiz</button>
        </div>
      </div>
      <div class="speaking-indicator hidden" id="speaking">
        <div class="speaking-dots">
          <span></span><span></span><span></span>
        </div>
        <span>Speaking...</span>
      </div>
    </div>
    
    <div class="chat">
      <div class="mode-toggle">
        <div id="voice-status" class="mode-btn active" style="flex:1; cursor:default; text-align:center;">
          üé§ Always Listening
        </div>
      </div>
      <div class="messages" id="messages"></div>
      <div class="input-area" id="tutor-input">
        <button id="mic-btn" class="mic-btn recording" onclick="toggleMic()" title="Mute/Unmute">üé§</button>
        <input type="text" id="input" placeholder="üé§ Listening..." disabled>
        <button id="send-btn" onclick="sendMessage()" disabled>Send</button>
      </div>
    </div>
  </div>

  <script type="module">
    import StreamingAvatar, { AvatarQuality, StreamingEvents } from 'https://esm.sh/@heygen/streaming-avatar@latest';
    
    let avatar = null;
    let sessionId = null;
    let conversationHistory = [];
    let mediaRecorder = null;
    let audioChunks = [];
    
    const params = new URLSearchParams(window.location.search);
    const studentName = params.get('name') || 'Student';
    const backendUrl = params.get('backend') || 'http://127.0.0.1:8000';
    
    document.getElementById('student-name').textContent = studentName;
    
    window.closeWindow = function() {
      if (avatar) avatar.stopAvatar().catch(() => {});
      window.close();
    }
    
    function showLoading() {
      document.getElementById('loading').classList.remove('hidden');
      document.getElementById('error').classList.add('hidden');
    }
    
    function showError(msg) {
      document.getElementById('loading').classList.add('hidden');
      document.getElementById('error').classList.remove('hidden');
      document.getElementById('error-msg').textContent = msg;
    }
    
    function hideOverlays() {
      document.getElementById('loading').classList.add('hidden');
      document.getElementById('error').classList.add('hidden');
    }
    
    function addMessage(role, text) {
      const div = document.createElement('div');
      div.className = 'message ' + role;
      div.textContent = text;
      document.getElementById('messages').appendChild(div);
      document.getElementById('messages').scrollTop = 9999;
    }
    
    async function fetchToken() {
      const res = await fetch(backendUrl + '/api/heygen-token');
      if (!res.ok) throw new Error('Failed to get token');
      const data = await res.json();
      return data.token;
    }
    
    window.startAvatar = async function() {
      showLoading();
      
      try {
        const token = await fetchToken();
        console.log('Got token');
        
        avatar = new StreamingAvatar({ token });
        
        avatar.on(StreamingEvents.STREAM_READY, async (event) => {
          console.log('Stream ready');
          const video = document.getElementById('avatar-video');
          video.srcObject = event.detail;

          // Ensure audio plays by unmuting video element
          video.muted = false;
          video.volume = 1.0;

          video.play().catch(console.error);
          hideOverlays();

          document.getElementById('input').disabled = false;
          document.getElementById('send-btn').disabled = false;
          document.getElementById('mic-btn').disabled = false;

          // Greeting message
          const greeting = 'Hello ' + studentName + '! I\'m your AI Professor. I have context from the lecture, so just ask me anything!';
          addMessage('avatar', greeting);

          // Make avatar speak the greeting
          isSpeakingResponse = true;
          try {
            await avatar.speak({
              text: greeting,
              taskType: 'talk',
              taskMode: 'sync',
            });
          } catch (e) {
            console.error('Greeting speak error:', e);
          }

          // Auto-start listening after greeting
          isSpeakingResponse = false;
          console.log('üé§ Auto-starting voice recognition...');
          setTimeout(startListening, 500);
        });
        
        avatar.on(StreamingEvents.STREAM_DISCONNECTED, () => {
          console.log('Disconnected');
          showError('Connection lost');
        });
        
        avatar.on(StreamingEvents.AVATAR_START_TALKING, () => {
          document.getElementById('speaking').classList.remove('hidden');
          console.log('üó£Ô∏è Avatar started talking');
        });

        avatar.on(StreamingEvents.AVATAR_STOP_TALKING, () => {
          document.getElementById('speaking').classList.add('hidden');
          console.log('üó£Ô∏è Avatar stopped talking');
          // Resume normal listening if we were speaking
          if (isSpeakingResponse) {
            isSpeakingResponse = false;
            interruptTranscript = '';
            setTimeout(startListening, 300);
          }
        });
        
        const session = await avatar.createStartAvatar({
          quality: AvatarQuality.Medium,
          avatarName: 'default',
          knowledgeBase: 'talking_photo',  // Required for instant/photo avatars
        });
        
        console.log('Session started:', session);
        sessionId = session.session_id;
        
      } catch (err) {
        console.error('Full error:', err);
        console.error('Error details:', JSON.stringify(err, null, 2));
        showError(err.message || 'Failed to connect');
      }
    }
    
    window.sendMessage = async function() {
      const input = document.getElementById('input');
      const text = input.value.trim();
      if (!text || !avatar) return;
      
      input.value = '';
      addMessage('student', text);
      conversationHistory.push({ role: 'student', text });
      
      try {
        let response;
        
        if (currentMode === 'voice') {
          // Custom voice mode: LLM + TTS pipeline, then feed audio to HeyGen
          const audioRes = await fetch(backendUrl + '/api/tutor-audio', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              message: text,
              student_name: studentName,
              history: conversationHistory,
              tts_provider: 'openai',  // Change to 'elevenlabs' for cloned voice
              // voice_id: 'YOUR_ELEVENLABS_VOICE_ID'  // Uncomment for custom voice
            })
          });
          
          if (audioRes.ok) {
            const data = await audioRes.json();
            response = data.response;
            
            if (data.audio_url) {
              // HeyGen lip-syncs to our custom audio
              const fullAudioUrl = backendUrl + data.audio_url;
              console.log('Speaking with custom audio:', fullAudioUrl);
              
              await avatar.speak({
                audio: fullAudioUrl,
                taskType: 'talk',
                taskMode: 'sync',
              });
            } else {
              // Fallback to HeyGen TTS if audio generation failed
              await avatar.speak({ text: response, taskType: 'talk', taskMode: 'sync' });
            }
          } else {
            response = 'Sorry, I had trouble processing that. Could you try again?';
            await avatar.speak({ text: response, taskType: 'talk', taskMode: 'sync' });
          }
        } else {
          // Tutor mode: Use HeyGen's built-in TTS
          const llmRes = await fetch(backendUrl + '/api/tutor-response', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              message: text,
              student_name: studentName,
              history: conversationHistory
            })
          });
          
          if (llmRes.ok) {
            const data = await llmRes.json();
            response = data.response;
          } else {
            response = 'That is a great question about "' + text + '". Let me help you understand this better.';
          }
          
          await avatar.speak({
            text: response,
            taskType: 'talk',
            taskMode: 'sync',
          });
        }
        
        addMessage('avatar', response);
        conversationHistory.push({ role: 'avatar', text: response });
      } catch (err) {
        console.error('Speak error:', err);
      }
    }
    
    document.getElementById('input').addEventListener('keypress', (e) => {
      if (e.key === 'Enter') window.sendMessage();
    });

    // ============ ALWAYS-ON VOICE CONVERSATION (DEEPGRAM) ============
    const DEEPGRAM_API_KEY = 'a216e7ec6135ca418f0d51407e6162cf3f999a24';
    let isListening = false;
    let isSpeakingResponse = false;
    let wasInterrupted = false;
    let deepgramSocket = null;
    let mediaStream = null;
    let finalTranscript = '';
    let silenceTimeout = null;
    let interruptTranscript = '';  // Track speech during avatar talking

    async function setupVoiceRecognition() {
      console.log('üé§ Setting up Deepgram voice recognition...');
    }

    function updateVoiceStatus(status) {
      const statusEl = document.getElementById('voice-status');
      const micBtn = document.getElementById('mic-btn');

      switch(status) {
        case 'listening':
          statusEl.textContent = 'üé§ Listening...';
          statusEl.style.background = '#10b981';
          micBtn.classList.add('recording');
          break;
        case 'thinking':
          statusEl.textContent = 'ü§î Thinking...';
          statusEl.style.background = '#f59e0b';
          micBtn.classList.remove('recording');
          break;
        case 'speaking':
          statusEl.textContent = 'üó£Ô∏è Speaking (interrupt anytime)';
          statusEl.style.background = '#3b82f6';
          micBtn.classList.add('recording');  // Keep mic indicator on
          break;
        case 'muted':
          statusEl.textContent = 'üîá Muted';
          statusEl.style.background = '#6b7280';
          micBtn.classList.remove('recording');
          break;
      }
    }

    async function startListening() {
      if (isListening || isSpeakingResponse) return;

      try {
        console.log('üé§ Starting Deepgram...');

        // Get microphone - use default sample rate, we'll resample if needed
        mediaStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });

        console.log('üé§ Got microphone stream');

        // Use MediaRecorder with webm/opus - Deepgram handles this natively
        const mediaRecorder = new MediaRecorder(mediaStream, {
          mimeType: 'audio/webm;codecs=opus'
        });

        // Connect to Deepgram - optimized for speed
        const dgUrl = 'wss://api.deepgram.com/v1/listen?' + new URLSearchParams({
          model: 'nova-2',
          language: 'en-US',
          smart_format: 'true',
          interim_results: 'true',
          utterance_end_ms: '800',      // Faster end detection
          vad_events: 'true',
          endpointing: '200',           // Faster endpointing
          punctuate: 'true'
        }).toString();

        console.log('üîó Connecting to Deepgram...');
        deepgramSocket = new WebSocket(dgUrl, ['token', DEEPGRAM_API_KEY]);

        deepgramSocket.onopen = () => {
          console.log('‚úÖ Deepgram connected! Starting recording...');
          isListening = true;
          finalTranscript = '';
          updateVoiceStatus('listening');

          // Send audio chunks to Deepgram
          mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0 && deepgramSocket?.readyState === WebSocket.OPEN) {
              deepgramSocket.send(event.data);
            }
          };

          // Start recording with 100ms chunks for lower latency
          mediaRecorder.start(100);
          window._mediaRecorder = mediaRecorder;
        };

        deepgramSocket.onmessage = async (msg) => {
          const data = JSON.parse(msg.data);

          // Handle utterance end (user stopped speaking)
          if (data.type === 'UtteranceEnd') {
            console.log('üîá Utterance ended');
            const textToProcess = isSpeakingResponse ? interruptTranscript.trim() : finalTranscript.trim();
            if (textToProcess) {
              if (isSpeakingResponse) {
                // This was an interrupt - stop avatar and process
                console.log('‚ö° Processing interrupt:', textToProcess);
                await handleInterrupt(textToProcess);
                interruptTranscript = '';
              } else {
                processVoiceInput(textToProcess, false);
              }
              finalTranscript = '';
            }
            return;
          }

          const transcript = data.channel?.alternatives?.[0]?.transcript || '';
          if (!transcript) return;

          // Check if this is an interrupt (user speaking while avatar talks)
          if (isSpeakingResponse) {
            console.log(`‚ö° INTERRUPT detected: "${transcript}"`);
            if (data.is_final) {
              interruptTranscript += ' ' + transcript;
              // Trigger interrupt immediately on any final transcript
              console.log('‚ö° Triggering interrupt NOW');
              await handleInterrupt(interruptTranscript.trim());
              interruptTranscript = '';
            } else {
              // Show interim in input field
              document.getElementById('input').value = '‚ö° ' + transcript;
            }
            return;
          }

          console.log(`üé§ ${data.is_final ? 'FINAL' : 'interim'}: "${transcript}"`);

          if (data.is_final) {
            finalTranscript += ' ' + transcript;
            document.getElementById('input').value = finalTranscript.trim();

            // Clear any existing timeout
            clearTimeout(silenceTimeout);

            // Set timeout to process after 800ms of silence (faster response)
            silenceTimeout = setTimeout(() => {
              if (finalTranscript.trim() && !isSpeakingResponse) {
                processVoiceInput(finalTranscript.trim(), false);
                finalTranscript = '';
              }
            }, 800);
          } else {
            // Show interim
            document.getElementById('input').value = finalTranscript + ' ' + transcript;
          }
        };

        deepgramSocket.onerror = (e) => {
          console.error('‚ùå Deepgram error:', e);
          updateVoiceStatus('muted');
        };

        deepgramSocket.onclose = (e) => {
          console.log('Deepgram closed:', e.code, e.reason);
          isListening = false;
          if (window._mediaRecorder?.state === 'recording') {
            window._mediaRecorder.stop();
          }
          // Always try to restart unless explicitly muted
          if (document.getElementById('mic-btn').textContent !== 'üîá') {
            console.log('üîÑ Restarting Deepgram in 1s...');
            setTimeout(startListening, 1000);
          }
        };

      } catch (e) {
        console.error('‚ùå Failed to start listening:', e);
        updateVoiceStatus('muted');
      }
    }

    function stopListening() {
      console.log('üõë Stopping listening...');
      isListening = false;
      clearTimeout(silenceTimeout);

      if (window._mediaRecorder?.state === 'recording') {
        window._mediaRecorder.stop();
        window._mediaRecorder = null;
      }
      if (mediaStream) {
        mediaStream.getTracks().forEach(t => t.stop());
        mediaStream = null;
      }
      if (deepgramSocket?.readyState === WebSocket.OPEN) {
        deepgramSocket.close();
      }
      deepgramSocket = null;
    }

    // Handle interrupt - INSTANTLY stop avatar audio and process new input
    async function handleInterrupt(text) {
      if (!text || !avatar) return;

      console.log('‚ö° INTERRUPT:', text);

      // INSTANT: Mute the video element to stop audio immediately
      const video = document.getElementById('avatar-video');
      if (video) video.muted = true;

      // Try to stop avatar via API (but don't wait for it)
      avatar.interrupt().catch(() => {});

      // Unmute after a brief moment (avatar will have new speech)
      setTimeout(() => {
        if (video) video.muted = false;
      }, 500);

      // Process the interrupt immediately
      isSpeakingResponse = false;
      interruptTranscript = '';
      finalTranscript = '';
      processVoiceInput(text, true);
    }

    async function processVoiceInput(text, wasInterrupted = false) {
      if (!text || !avatar) return;
      if (isSpeakingResponse && !wasInterrupted) return;

      console.log('üìù Processing:', text, wasInterrupted ? '(interrupt)' : '');

      // Set speaking flag but DON'T stop listening - we want to hear interrupts
      isSpeakingResponse = true;
      updateVoiceStatus('thinking');

      addMessage('student', text);
      conversationHistory.push({ role: 'student', text });
      document.getElementById('input').value = '';

      try {
        // Get LLM response (with meeting context!)
        console.log('üß† Calling LLM...', wasInterrupted ? '(interrupt mode)' : '');
        const llmRes = await fetch(backendUrl + '/api/tutor-response', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            message: text,
            student_name: studentName,
            history: conversationHistory,
            was_interrupted: wasInterrupted
          })
        });

        let response;
        if (llmRes.ok) {
          const data = await llmRes.json();
          response = data.response;
          console.log('ü§ñ LLM response:', response);
        } else {
          response = "I'm sorry, I had trouble understanding. Could you repeat that?";
        }

        addMessage('avatar', response);
        conversationHistory.push({ role: 'avatar', text: response });

        updateVoiceStatus('speaking');

        // Make avatar speak
        console.log('üó£Ô∏è Avatar speaking...');
        await avatar.speak({
          text: response,
          taskType: 'talk',
          taskMode: 'sync',
        });
        console.log('‚úÖ Avatar done speaking');

      } catch (err) {
        console.error('‚ùå Voice processing error:', err);
        addMessage('avatar', "Sorry, there was an error. Please try again.");
        // Reset state on error
        isSpeakingResponse = false;
        updateVoiceStatus('listening');
      }
      // Note: isSpeakingResponse is reset by AVATAR_STOP_TALKING event
    }

    // Toggle mic button (mute/unmute)
    window.toggleMic = function() {
      if (isListening) {
        stopListening();
        isSpeakingResponse = true;  // Prevent auto-restart
        updateVoiceStatus('muted');
        document.getElementById('mic-btn').textContent = 'üîá';
      } else {
        isSpeakingResponse = false;
        document.getElementById('mic-btn').textContent = 'üé§';
        startListening();
      }
    }

    // Start avatar and auto-enable voice
    window.startAvatar();

    // ============ EXPLAINER VIDEO HANDLING ============
    let currentExplainerJid = null;
    const explainerOverlay = document.getElementById('explainer-overlay');
    const explainerVideo = document.getElementById('explainer-video');
    const explainerTitle = document.getElementById('explainer-title');
    const explainerConcept = document.getElementById('explainer-concept');
    const continueBtn = document.getElementById('continue-btn');

    // Listen for play-explainer-video message from Electron main process
    // Since this is loaded with contextIsolation: false, we can use window events
    window.addEventListener('message', (event) => {
      if (event.data && event.data.type === 'play-explainer-video') {
        playExplainerVideo(event.data.payload);
      }
    });

    // Also listen via electron IPC if available
    if (typeof require !== 'undefined') {
      try {
        const { ipcRenderer } = require('electron');
        ipcRenderer.on('play-explainer-video', (event, payload) => {
          playExplainerVideo(payload);
        });
      } catch (e) {
        console.log('IPC not available, using postMessage');
      }
    }

    function playExplainerVideo(payload) {
      console.log('üé¨ Playing explainer video:', payload);

      const { concept, video_url, video_path, student_jid } = payload;
      currentExplainerJid = student_jid;

      // Update UI
      explainerTitle.textContent = 'Explainer: ' + (concept || 'Concept');
      explainerConcept.textContent = 'Watch this video to understand the concept better';

      // Determine video URL
      let videoSrc = video_url;
      if (!videoSrc && video_path) {
        // Use local file path
        videoSrc = video_path.startsWith('/') ? 'file://' + video_path : backendUrl + video_path;
      }

      if (!videoSrc) {
        console.error('No video source provided');
        addMessage('avatar', "Sorry, the explainer video isn't available right now.");
        notifyVideoCompleted();
        return;
      }

      // Pause avatar audio
      const avatarVideoEl = document.getElementById('avatar-video');
      if (avatarVideoEl) avatarVideoEl.muted = true;

      // Stop listening while watching video
      stopListening();

      // Show overlay and play video
      explainerVideo.src = videoSrc;
      explainerOverlay.classList.remove('hidden');
      continueBtn.classList.add('hidden');

      explainerVideo.play().catch(e => {
        console.error('Failed to play video:', e);
        addMessage('avatar', "Couldn't play the video. Let's continue.");
        hideExplainerAndContinue();
      });

      // Show continue button when video ends
      explainerVideo.onended = () => {
        console.log('üé¨ Explainer video ended');
        continueBtn.classList.remove('hidden');
      };

      explainerVideo.onerror = () => {
        console.error('Video error');
        addMessage('avatar', "There was an issue with the video. Let's continue.");
        hideExplainerAndContinue();
      };
    }

    window.skipExplainer = function() {
      console.log('‚è≠Ô∏è Skipping explainer video');
      explainerVideo.pause();
      hideExplainerAndContinue();
    };

    window.continueAfterExplainer = function() {
      console.log('‚û°Ô∏è Continuing after explainer');
      hideExplainerAndContinue();
    };

    function hideExplainerAndContinue() {
      explainerOverlay.classList.add('hidden');
      explainerVideo.pause();
      explainerVideo.src = '';

      // Unmute avatar
      const avatarVideoEl = document.getElementById('avatar-video');
      if (avatarVideoEl) avatarVideoEl.muted = false;

      // Notify backend that video is done
      notifyVideoCompleted();

      // Resume listening
      setTimeout(startListening, 500);
    }

    async function notifyVideoCompleted() {
      if (!currentExplainerJid) return;

      try {
        const response = await fetch(backendUrl + '/api/quiz/video-completed', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ student_jid: currentExplainerJid })
        });

        if (response.ok) {
          console.log('‚úÖ Notified backend video completed');
        }
      } catch (e) {
        console.error('Failed to notify video completion:', e);
      }

      currentExplainerJid = null;
    }
  </script>
</body>
</html>
