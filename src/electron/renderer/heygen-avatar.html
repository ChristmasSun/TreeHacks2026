<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Tutor</title>
  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: linear-gradient(135deg, #111827 0%, #1f2937 50%, #000 100%);
      color: white;
      height: 100vh;
      overflow: hidden;
      position: relative;
    }
    /* Full-screen video */
    .video-container {
      position: absolute;
      inset: 0;
    }
    #avatar-video {
      width: 100%;
      height: 100%;
      object-fit: cover;
      transition: opacity 0.5s;
    }
    /* Loading / Error overlays */
    .loading, .error {
      position: absolute;
      inset: 0;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      background: linear-gradient(135deg, #111827 0%, #1f2937 50%, #000 100%);
    }
    .spinner {
      width: 48px;
      height: 48px;
      border: 2px solid rgba(96, 165, 250, 0.3);
      border-top-color: #60a5fa;
      border-radius: 50%;
      animation: spin 1s linear infinite;
    }
    @keyframes spin { to { transform: rotate(360deg); } }
    .loading p { margin-top: 16px; opacity: 0.6; font-size: 14px; font-weight: 300; }
    .error {
      color: white;
    }
    .error-card {
      text-align: center;
      padding: 24px;
      background: rgba(255,255,255,0.04);
      backdrop-filter: blur(20px);
      -webkit-backdrop-filter: blur(20px);
      border: 1px solid rgba(255,255,255,0.08);
      border-radius: 16px;
      max-width: 360px;
    }
    .error-card .error-title { color: rgba(248,113,113,0.8); margin-bottom: 8px; font-weight: 500; }
    .error-card .error-msg { color: rgba(255,255,255,0.4); font-size: 14px; font-weight: 300; margin-bottom: 16px; }
    .retry-btn {
      padding: 10px 24px;
      background: rgba(59,130,246,0.2);
      border: 1px solid rgba(59,130,246,0.2);
      border-radius: 12px;
      color: #93c5fd;
      cursor: pointer;
      font-size: 14px;
      font-weight: 500;
      transition: all 0.2s;
    }
    .retry-btn:hover { background: rgba(59,130,246,0.3); }
    /* Session label pill */
    .session-label {
      position: absolute;
      top: 24px;
      left: 50%;
      transform: translateX(-50%);
      background: rgba(255,255,255,0.06);
      backdrop-filter: blur(20px);
      -webkit-backdrop-filter: blur(20px);
      border: 1px solid rgba(255,255,255,0.08);
      padding: 6px 16px;
      border-radius: 9999px;
      z-index: 20;
    }
    .session-label span {
      color: rgba(255,255,255,0.6);
      font-size: 14px;
      font-weight: 300;
      letter-spacing: 0.025em;
    }
    /* Speaking indicator */
    .speaking-indicator {
      position: absolute;
      top: 24px;
      left: 50%;
      transform: translateX(-50%);
      background: rgba(255,255,255,0.06);
      backdrop-filter: blur(20px);
      -webkit-backdrop-filter: blur(20px);
      border: 1px solid rgba(255,255,255,0.1);
      padding: 6px 16px;
      border-radius: 9999px;
      font-size: 12px;
      display: flex;
      align-items: center;
      gap: 8px;
      z-index: 20;
    }
    .speaking-indicator span:last-child { color: rgba(255,255,255,0.6); font-weight: 300; }
    .speaking-dots {
      display: flex;
      gap: 3px;
    }
    .speaking-dots span {
      width: 6px;
      height: 6px;
      background: #60a5fa;
      border-radius: 50%;
      animation: pulse 1s infinite;
    }
    .speaking-dots span:nth-child(2) { animation-delay: 0.1s; }
    .speaking-dots span:nth-child(3) { animation-delay: 0.2s; }
    @keyframes pulse {
      0%, 100% { opacity: 0.4; transform: scale(0.8); }
      50% { opacity: 1; transform: scale(1); }
    }
    /* Bottom controls */
    .bottom-controls {
      position: absolute;
      bottom: 32px;
      left: 50%;
      transform: translateX(-50%);
      display: flex;
      align-items: center;
      gap: 16px;
      z-index: 20;
    }
    .ctrl-btn {
      width: 56px;
      height: 56px;
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      backdrop-filter: blur(20px);
      -webkit-backdrop-filter: blur(20px);
      border: 1px solid rgba(255,255,255,0.1);
      cursor: pointer;
      transition: all 0.2s;
    }
    .ctrl-btn svg { width: 24px; height: 24px; }
    .mute-btn {
      background: rgba(255,255,255,0.08);
    }
    .mute-btn:hover { background: rgba(255,255,255,0.14); }
    .mute-btn.muted {
      background: rgba(239,68,68,0.3);
      border-color: rgba(239,68,68,0.3);
    }
    .mute-btn.muted:hover { background: rgba(239,68,68,0.4); }
    .exit-btn {
      background: rgba(239,68,68,0.2);
      border-color: rgba(239,68,68,0.3);
    }
    .exit-btn:hover { background: rgba(239,68,68,0.3); }
    .hidden { display: none !important; }
    /* Voice status pill (small, top-left) */
    .voice-status {
      position: absolute;
      top: 24px;
      left: 24px;
      background: rgba(255,255,255,0.06);
      backdrop-filter: blur(20px);
      -webkit-backdrop-filter: blur(20px);
      border: 1px solid rgba(255,255,255,0.08);
      padding: 4px 12px;
      border-radius: 9999px;
      font-size: 12px;
      font-weight: 300;
      color: rgba(255,255,255,0.5);
      z-index: 20;
      transition: all 0.2s;
    }
    /* Explainer video overlay */
    .explainer-overlay {
      position: absolute;
      inset: 0;
      background: rgba(0,0,0,0.95);
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      z-index: 100;
    }
    .explainer-overlay.hidden { display: none; }
    .explainer-header {
      padding: 12px 20px;
      text-align: center;
    }
    .explainer-header h2 {
      font-size: 18px;
      font-weight: 500;
      margin-bottom: 4px;
    }
    .explainer-header p {
      font-size: 14px;
      opacity: 0.7;
      font-weight: 300;
    }
    #explainer-video {
      max-width: 100%;
      max-height: calc(100% - 100px);
      border-radius: 12px;
    }
    .explainer-controls {
      padding: 12px 20px;
      display: flex;
      gap: 12px;
    }
    .explainer-btn {
      padding: 10px 24px;
      border: none;
      border-radius: 12px;
      cursor: pointer;
      font-size: 14px;
      transition: all 0.2s;
    }
    .explainer-btn.primary {
      background: rgba(59,130,246,0.2);
      border: 1px solid rgba(59,130,246,0.2);
      color: #93c5fd;
    }
    .explainer-btn.primary:hover { background: rgba(59,130,246,0.3); }
    .explainer-btn.secondary {
      background: rgba(255,255,255,0.06);
      border: 1px solid rgba(255,255,255,0.08);
      color: rgba(255,255,255,0.6);
    }
    .explainer-btn.secondary:hover { background: rgba(255,255,255,0.1); }
  </style>
</head>
<body>
  <!-- Full-screen video -->
  <div class="video-container">
    <div class="loading" id="loading">
      <div class="spinner"></div>
      <p>Connecting to AI Professor...</p>
    </div>
    <div class="error hidden" id="error">
      <div class="error-card">
        <div class="error-title">Connection Error</div>
        <div class="error-msg" id="error-msg"></div>
        <button class="retry-btn" onclick="startAvatar()">Retry Connection</button>
      </div>
    </div>
    <video id="avatar-video" autoplay playsinline></video>
    <!-- Explainer video overlay for quiz -->
    <div class="explainer-overlay hidden" id="explainer-overlay">
      <div class="explainer-header">
        <h2 id="explainer-title">Explainer Video</h2>
        <p id="explainer-concept">Watch to learn more about this concept</p>
      </div>
      <video id="explainer-video" controls></video>
      <div class="explainer-controls">
        <button class="explainer-btn secondary" onclick="skipExplainer()">Skip</button>
        <button class="explainer-btn primary hidden" id="continue-btn" onclick="continueAfterExplainer()">Continue Quiz</button>
      </div>
    </div>
  </div>

  <!-- Session label -->
  <div class="session-label hidden" id="session-label">
    <span>AI Professor &middot; <span id="student-name">Student</span></span>
  </div>

  <!-- Speaking indicator -->
  <div class="speaking-indicator hidden" id="speaking">
    <div class="speaking-dots">
      <span></span><span></span><span></span>
    </div>
    <span>Speaking...</span>
  </div>

  <!-- Voice status pill -->
  <div class="voice-status" id="voice-status">Connecting...</div>

  <!-- Bottom controls: Mute + Exit -->
  <div class="bottom-controls">
    <button id="mic-btn" class="ctrl-btn mute-btn" onclick="toggleMic()" title="Mute/Unmute">
      <svg fill="none" stroke="rgba(255,255,255,0.8)" viewBox="0 0 24 24" stroke-width="1.5">
        <path stroke-linecap="round" stroke-linejoin="round" d="M12 18.75a6 6 0 0 0 6-6v-1.5m-6 7.5a6 6 0 0 1-6-6v-1.5m6 7.5v3.75m-3.75 0h7.5M12 15.75a3 3 0 0 1-3-3V4.5a3 3 0 1 1 6 0v8.25a3 3 0 0 1-3 3Z" />
      </svg>
    </button>
    <button class="ctrl-btn exit-btn" onclick="closeWindow()" title="Exit">
      <svg fill="none" stroke="rgba(252,165,165,1)" viewBox="0 0 24 24" stroke-width="1.5">
        <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 9V5.25A2.25 2.25 0 0 0 13.5 3h-6a2.25 2.25 0 0 0-2.25 2.25v13.5A2.25 2.25 0 0 0 7.5 21h6a2.25 2.25 0 0 0 2.25-2.25V15m3 0 3-3m0 0-3-3m3 3H9" />
      </svg>
    </button>
  </div>

  <!-- Hidden elements for JS compat (messages log kept in memory, not displayed) -->
  <div id="messages" class="hidden"></div>
  <input type="hidden" id="input">
  <button id="send-btn" class="hidden"></button>

  <script type="module">
    import { Room, RoomEvent } from 'https://esm.sh/livekit-client@latest';

    let livekitRoom = null;
    let sessionId = null;
    let heygenWsUrl = null;   // Passed to backend for LITE audio control
    let sessionToken = null;  // Session token for LITE WebSocket auth + session stop
    let conversationHistory = [];

    const params = new URLSearchParams(window.location.search);
    const studentName = params.get('name') || 'Student';
    const rawBackend = params.get('backend') || 'http://127.0.0.1:8000';
    // Validate backend URL is a safe HTTP(S) origin
    const backendUrl = (() => {
      try {
        const u = new URL(rawBackend);
        if (u.protocol === 'http:' || u.protocol === 'https:') return u.origin;
      } catch {}
      return 'http://127.0.0.1:8000';
    })();

    document.getElementById('student-name').textContent = studentName;

    window.closeWindow = function() {
      if (livekitRoom) livekitRoom.disconnect().catch(() => {});
      window.close();
    }

    function showLoading() {
      document.getElementById('loading').classList.remove('hidden');
      document.getElementById('error').classList.add('hidden');
      document.getElementById('session-label').classList.add('hidden');
    }

    function showError(msg) {
      document.getElementById('loading').classList.add('hidden');
      document.getElementById('error').classList.remove('hidden');
      document.getElementById('error-msg').textContent = msg;
      document.getElementById('session-label').classList.add('hidden');
    }

    function hideOverlays() {
      document.getElementById('loading').classList.add('hidden');
      document.getElementById('error').classList.add('hidden');
      document.getElementById('session-label').classList.remove('hidden');
    }

    function addMessage(role, text) {
      const div = document.createElement('div');
      div.className = 'message ' + role;
      div.textContent = text;
      document.getElementById('messages').appendChild(div);
      document.getElementById('messages').scrollTop = 9999;
    }

    window.startAvatar = async function() {
      showLoading();

      try {
        // 1. Create LITE session via backend
        console.log('Fetching HeyGen LITE session...');
        const res = await fetch(backendUrl + '/api/heygen-token');
        if (!res.ok) {
          const errText = await res.text();
          throw new Error('HeyGen session failed: ' + errText);
        }
        const sessionData = await res.json();
        console.log('Session data:', JSON.stringify(sessionData, null, 2));

        sessionId = sessionData.session_id;
        heygenWsUrl = sessionData.ws_url;
        sessionToken = sessionData.session_token;
        const livekitUrl = sessionData.livekit_url;
        const accessToken = sessionData.livekit_token;

        console.log('Got LITE session:', sessionId, 'ws_url:', heygenWsUrl ? 'yes' : 'MISSING', 'livekit:', livekitUrl ? 'yes' : 'MISSING');

        // Start audio pipeline immediately (don't wait for video)
        // Kill any stale reconnect loops, then start fresh
        shouldReconnect = false;
        stopAudioPipeline();

        const greeting = 'Hello ' + studentName + '! I\'m your AI Professor. I have context from the lecture, so just ask me anything!';
        addMessage('avatar', greeting);

        shouldReconnect = true;
        startAudioPipeline();

        if (!livekitUrl || !accessToken) {
          console.warn('Missing LiveKit credentials ‚Äî video will not load, but audio still works');
          return;
        }

        // 2. Connect to LiveKit for video rendering
        livekitRoom = new Room({
          adaptiveStream: true,
          dynacast: true,
        });

        livekitRoom.on(RoomEvent.TrackSubscribed, (track, publication, participant) => {
          console.log('Track subscribed:', track.kind, participant.identity);
          const video = document.getElementById('avatar-video');

          if (track.kind === 'video') {
            track.attach(video);
            video.muted = false;
            video.volume = 1.0;
            hideOverlays();
          }

          if (track.kind === 'audio') {
            track.attach(video);
          }
        });

        livekitRoom.on(RoomEvent.Disconnected, () => {
          console.log('LiveKit disconnected');
        });

        await livekitRoom.connect(livekitUrl, accessToken);
        console.log('Connected to LiveKit room');

        // Check for existing tracks (in case they arrived before our listener)
        livekitRoom.remoteParticipants.forEach((participant) => {
          participant.trackPublications.forEach((publication) => {
            if (publication.track) {
              const video = document.getElementById('avatar-video');
              if (publication.track.kind === 'video') {
                publication.track.attach(video);
                hideOverlays();
              }
              if (publication.track.kind === 'audio') {
                publication.track.attach(video);
              }
            }
          });
        });

      } catch (err) {
        console.error('Avatar start error:', err);
        showError(err.message || 'Failed to connect');
      }
    }

    window.sendMessage = async function() {
      const input = document.getElementById('input');
      const text = input.value.trim();
      if (!text) return;

      input.value = '';
      addMessage('student', text);
      conversationHistory.push({ role: 'student', text });

      try {
        const llmRes = await fetch(backendUrl + '/api/tutor-response', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            message: text,
            student_name: studentName,
            history: conversationHistory
          })
        });

        let response;
        if (llmRes.ok) {
          const data = await llmRes.json();
          response = data.response;
        } else {
          response = 'Sorry, I had trouble processing that. Could you try again?';
        }

        addMessage('avatar', response);
        conversationHistory.push({ role: 'avatar', text: response });
        // TTS + lip-sync handled by backend via LITE mode (no avatarSpeak needed)
      } catch (err) {
        console.error('Send error:', err);
      }
    }

    // Text input removed ‚Äî voice-only interface

    // ============ VOICE PIPELINE (AudioWorklet ‚Üí Backend WebSocket ‚Üí Silero VAD + Deepgram nova-3) ============
    let isListening = false;
    let isSpeakingResponse = false;
    let isMuted = false;
    let shouldReconnect = false;  // Only reconnect after startAvatar succeeds
    let audioWs = null;
    let audioContext = null;
    let audioWorkletNode = null;
    let mediaStream = null;

    function updateVoiceStatus(status) {
      const statusEl = document.getElementById('voice-status');
      const micBtn = document.getElementById('mic-btn');

      switch(status) {
        case 'listening':
          statusEl.textContent = 'Listening...';
          statusEl.style.color = '#6ee7b7';
          micBtn.classList.remove('muted');
          break;
        case 'thinking':
          statusEl.textContent = 'Thinking...';
          statusEl.style.color = '#fcd34d';
          micBtn.classList.remove('muted');
          break;
        case 'speaking':
          statusEl.textContent = 'Speaking';
          statusEl.style.color = '#93c5fd';
          micBtn.classList.remove('muted');
          break;
        case 'muted':
          statusEl.textContent = 'Muted';
          statusEl.style.color = 'rgba(255,255,255,0.4)';
          micBtn.classList.add('muted');
          break;
      }
    }

    // AudioWorklet processor with resampling support (data URI for Electron compat)
    const pcmProcessorCode = `
      class PCMProcessor extends AudioWorkletProcessor {
        constructor(options) {
          super();
          // Target: 512 samples at 16kHz (32ms chunks)
          this.targetRate = 16000;
          this.sourceRate = options.processorOptions?.sampleRate || sampleRate;
          this.ratio = this.sourceRate / this.targetRate;
          this.buffer = new Float32Array(512);
          this.bufferIndex = 0;
          this.srcIndex = 0; // fractional source index for resampling
        }
        process(inputs) {
          const input = inputs[0]?.[0];
          if (!input) return true;

          if (Math.abs(this.ratio - 1.0) < 0.01) {
            // No resampling needed (source is already 16kHz)
            for (let i = 0; i < input.length; i++) {
              this.buffer[this.bufferIndex++] = input[i];
              if (this.bufferIndex === 512) {
                this.port.postMessage(this.buffer.slice());
                this.bufferIndex = 0;
              }
            }
          } else {
            // Linear interpolation downsampling
            for (let i = 0; i < input.length; i++) {
              this.srcIndex += 1;
              while (this.srcIndex >= this.ratio) {
                this.srcIndex -= this.ratio;
                // Simple nearest-neighbor for speed
                this.buffer[this.bufferIndex++] = input[i];
                if (this.bufferIndex === 512) {
                  this.port.postMessage(this.buffer.slice());
                  this.bufferIndex = 0;
                }
              }
            }
          }
          return true;
        }
      }
      registerProcessor('pcm-processor', PCMProcessor);
    `;
    // Use data URI ‚Äî Blob URLs can fail in file:// or Electron contexts
    const processorDataUrl = 'data:application/javascript;base64,' + btoa(pcmProcessorCode);

    async function startAudioPipeline() {
      // Kill any existing connection first
      if (audioWs) {
        const old = audioWs;
        audioWs = null;
        try { old.onclose = null; old.close(); } catch(e) {}
      }

      console.log('Starting audio pipeline... heygenWsUrl=' + (heygenWsUrl ? 'set' : 'MISSING'));

      try {
        // 1. Connect WebSocket to backend audio endpoint
        const wsUrl = backendUrl.replace('http', 'ws') + '/ws/audio';
        audioWs = new WebSocket(wsUrl);

        audioWs.onopen = () => {
          console.log('Audio WebSocket connected');
          audioWs.send(JSON.stringify({
            type: 'start_session',
            student_name: studentName,
            meeting_id: params.get('meeting_id'),
            heygen_ws_url: heygenWsUrl,    // Backend connects to LITE WebSocket
            session_token: sessionToken,    // Auth for LITE session
          }));
        };

        audioWs.onmessage = async (event) => {
          const data = JSON.parse(event.data);

          switch (data.type) {
            case 'interim_transcript':
              // Log live transcription (no visible input field)
              console.log('Transcript:', data.text);
              break;

            case 'vad_speech_start':
              if (!isSpeakingResponse) {
                updateVoiceStatus('listening');
              }
              break;

            case 'vad_speech_end':
              // Backend processed: VAD end ‚Üí LLM response ‚Üí PocketTTS ‚Üí HeyGen LITE
              console.log('Got response:', data.response);

              addMessage('student', data.transcript);
              addMessage('avatar', data.response);
              conversationHistory.push({ role: 'student', text: data.transcript });
              conversationHistory.push({ role: 'avatar', text: data.response });

              updateVoiceStatus('speaking');
              isSpeakingResponse = true;
              document.getElementById('speaking').classList.remove('hidden');
              // Backend handles TTS + HeyGen LITE audio ‚Äî no avatarSpeak() needed
              break;

            case 'avatar_done':
              // Backend signals avatar finished speaking (via HeyGen LITE speak_ended)
              console.log('Avatar done speaking');
              isSpeakingResponse = false;
              document.getElementById('speaking').classList.add('hidden');
              updateVoiceStatus('listening');
              break;

            case 'interrupt_detected':
              // Backend VAD detected speech during avatar playback
              console.log('Interrupt detected');
              isSpeakingResponse = false;
              document.getElementById('speaking').classList.add('hidden');
              updateVoiceStatus('listening');
              break;

            case 'error':
              console.error('Backend error:', data.message);
              break;
          }
        };

        audioWs.onerror = (e) => {
          console.error('Audio WebSocket error:', e);
        };

        audioWs.onclose = (e) => {
          console.log('Audio WebSocket closed:', e.code);
          isListening = false;
          if (shouldReconnect && !isMuted) {
            console.log('Reconnecting audio pipeline in 2s...');
            setTimeout(startAudioPipeline, 2000);
          }
        };

        // 2. Set up AudioWorklet for PCM capture (resample to 16kHz in worklet)
        audioContext = new AudioContext();
        console.log('AudioContext sample rate:', audioContext.sampleRate);
        mediaStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          }
        });

        const source = audioContext.createMediaStreamSource(mediaStream);
        await audioContext.audioWorklet.addModule(processorDataUrl);
        audioWorkletNode = new AudioWorkletNode(audioContext, 'pcm-processor', {
          processorOptions: { sampleRate: audioContext.sampleRate }
        });

        // Convert float32 chunks to int16 and send via WebSocket
        audioWorkletNode.port.onmessage = (event) => {
          if (audioWs?.readyState !== WebSocket.OPEN || isMuted) return;
          const float32 = event.data;
          const int16 = new Int16Array(float32.length);
          for (let i = 0; i < float32.length; i++) {
            int16[i] = Math.max(-32768, Math.min(32767, Math.round(float32[i] * 32767)));
          }
          audioWs.send(int16.buffer);
        };

        source.connect(audioWorkletNode);
        // Connect to a silent destination to keep the worklet running
        audioWorkletNode.connect(audioContext.destination);

        isListening = true;
        updateVoiceStatus('listening');
        console.log('Audio pipeline started (16kHz PCM -> backend)');

      } catch (e) {
        console.error('Failed to start audio pipeline:', e);
        updateVoiceStatus('muted');
      }
    }

    function stopAudioPipeline() {
      console.log('Stopping audio pipeline...');
      isListening = false;

      if (audioWorkletNode) {
        audioWorkletNode.disconnect();
        audioWorkletNode = null;
      }
      if (mediaStream) {
        mediaStream.getTracks().forEach(t => t.stop());
        mediaStream = null;
      }
      if (audioContext) {
        audioContext.close().catch(() => {});
        audioContext = null;
      }
      if (audioWs) {
        audioWs.onclose = null;  // Prevent reconnect from firing
        try { audioWs.close(); } catch(e) {}
      }
      audioWs = null;
    }

    // Aliases for backward compat with explainer video code
    const startListening = startAudioPipeline;
    const stopListening = stopAudioPipeline;

    const micIconUnmuted = '<svg fill="none" stroke="rgba(255,255,255,0.8)" viewBox="0 0 24 24" stroke-width="1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 18.75a6 6 0 0 0 6-6v-1.5m-6 7.5a6 6 0 0 1-6-6v-1.5m6 7.5v3.75m-3.75 0h7.5M12 15.75a3 3 0 0 1-3-3V4.5a3 3 0 1 1 6 0v8.25a3 3 0 0 1-3 3Z" /></svg>';
    const micIconMuted = '<svg fill="none" stroke="rgba(252,165,165,1)" viewBox="0 0 24 24" stroke-width="1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19 19 17.591 17.591 5.409 5.409 4 4" /><path stroke-linecap="round" stroke-linejoin="round" d="M12 18.75a6 6 0 0 1-6-6v-1.5m12 1.5a5.97 5.97 0 0 1-.838 3.07M12 22.5v-3.75m0-11.25a3 3 0 0 0-3 3v4.5m6-1.5V7.5a3 3 0 0 0-5.116-2.12" /></svg>';

    // Toggle mic button (mute/unmute)
    window.toggleMic = function() {
      const micBtn = document.getElementById('mic-btn');
      if (isListening && !isMuted) {
        isMuted = true;
        stopAudioPipeline();
        updateVoiceStatus('muted');
        micBtn.innerHTML = micIconMuted;
      } else {
        isMuted = false;
        micBtn.innerHTML = micIconUnmuted;
        startAudioPipeline();
      }
    }

    // Start avatar and auto-enable voice
    window.startAvatar();

    // ============ EXPLAINER VIDEO HANDLING ============
    let currentExplainerJid = null;
    const explainerOverlay = document.getElementById('explainer-overlay');
    const explainerVideo = document.getElementById('explainer-video');
    const explainerTitle = document.getElementById('explainer-title');
    const explainerConcept = document.getElementById('explainer-concept');
    const continueBtn = document.getElementById('continue-btn');

    // Listen for play-explainer-video message from Electron main process
    // Since this is loaded with contextIsolation: false, we can use window events
    window.addEventListener('message', (event) => {
      // Only accept messages from same origin (Electron main process)
      if (event.origin !== 'file://' && event.origin !== window.location.origin) return;
      if (event.data && event.data.type === 'play-explainer-video') {
        playExplainerVideo(event.data.payload);
      }
    });

    // Also listen via electron IPC if available
    if (typeof require !== 'undefined') {
      try {
        const { ipcRenderer } = require('electron');
        ipcRenderer.on('play-explainer-video', (event, payload) => {
          playExplainerVideo(payload);
        });
      } catch (e) {
        console.log('IPC not available, using postMessage');
      }
    }

    function playExplainerVideo(payload) {
      console.log('üé¨ Playing explainer video:', payload);

      const { concept, video_url, video_path, student_jid } = payload;
      currentExplainerJid = student_jid;

      // Update UI
      explainerTitle.textContent = 'Explainer: ' + (concept || 'Concept');
      explainerConcept.textContent = 'Watch this video to understand the concept better';

      // Determine video URL
      let videoSrc = video_url;
      if (!videoSrc && video_path) {
        // Use local file path
        videoSrc = video_path.startsWith('/') ? 'file://' + video_path : backendUrl + video_path;
      }

      if (!videoSrc) {
        console.error('No video source provided');
        addMessage('avatar', "Sorry, the explainer video isn't available right now.");
        notifyVideoCompleted();
        return;
      }

      // Pause avatar audio
      const avatarVideoEl = document.getElementById('avatar-video');
      if (avatarVideoEl) avatarVideoEl.muted = true;

      // Stop listening while watching video
      stopListening();

      // Show overlay and play video
      explainerVideo.src = videoSrc;
      explainerOverlay.classList.remove('hidden');
      continueBtn.classList.add('hidden');

      explainerVideo.play().catch(e => {
        console.error('Failed to play video:', e);
        addMessage('avatar', "Couldn't play the video. Let's continue.");
        hideExplainerAndContinue();
      });

      // Show continue button when video ends
      explainerVideo.onended = () => {
        console.log('üé¨ Explainer video ended');
        continueBtn.classList.remove('hidden');
      };

      explainerVideo.onerror = () => {
        console.error('Video error');
        addMessage('avatar', "There was an issue with the video. Let's continue.");
        hideExplainerAndContinue();
      };
    }

    window.skipExplainer = function() {
      console.log('‚è≠Ô∏è Skipping explainer video');
      explainerVideo.pause();
      hideExplainerAndContinue();
    };

    window.continueAfterExplainer = function() {
      console.log('‚û°Ô∏è Continuing after explainer');
      hideExplainerAndContinue();
    };

    function hideExplainerAndContinue() {
      explainerOverlay.classList.add('hidden');
      explainerVideo.pause();
      explainerVideo.src = '';

      // Unmute avatar
      const avatarVideoEl = document.getElementById('avatar-video');
      if (avatarVideoEl) avatarVideoEl.muted = false;

      // Notify backend that video is done
      notifyVideoCompleted();

      // Resume listening
      setTimeout(startListening, 500);
    }

    async function notifyVideoCompleted() {
      if (!currentExplainerJid) return;

      try {
        const response = await fetch(backendUrl + '/api/quiz/video-completed', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ student_jid: currentExplainerJid })
        });

        if (response.ok) {
          console.log('‚úÖ Notified backend video completed');
        }
      } catch (e) {
        console.error('Failed to notify video completion:', e);
      }

      currentExplainerJid = null;
    }
  </script>
</body>
</html>
