. Morning and welcome back. So what we'll see today in class is the first in-depth discussion of a learning algorithm, linear regression. And in particular, over the next, what, hour and a bit, you'll see linear regression batched in stochastic gradient descent as an algorithm for fitting linear regression models. then the normal equations as a way of, as a very efficient way to let you fit linear models. And we're going to define notation and a few concepts today that will lay the foundation for a lot of the work that we'll see the rest of this quarter. So, to, to motivate linear regression, it's kind of, you know, maybe the, maybe the simplest, one of the simplest learning algorithms. You remember the Alvin video, the autonomous driving video that I had shown in class on Monday. Um, for the self-driving car video, that was a supervised learning problem, right? And, uh, the term supervised learning meant that you were given Xs, which was a picture of what's in front of the car, and the algorithm had to map that to an output Y, which was the steering direction. And, uh, that was a regression problem, because the output Y that you want is continuous value, right? As opposed to classification problem where Y is discrete. And we'll talk about classification, um, next Monday, but supervised learning regression. So I think the simplest, maybe the simplest possible learning algorithm, a supervised learning regression problem is linear regression. Um, and to motivate that, rather than using a, uh, self-driving car example, you know, which is quite complicated, we'll, we'll build up a supervised learning algorithm using a simpler example. So let's say you want to predict or estimate the prices of houses. So the way you build a learning algorithm is start by collecting a data set of houses and their prices. So this is a data set that we collected off Craigslist a little bit back. This is data from Portland, Oregon. So there's the size of a house in square feet. And that the price of a house in thousands of dollars And so there a house that is 2 square feet whose asking price was house with that size with that price and so on And maybe more conventionally, if you plot this data, there's the size, there's the price. So you have some data set like that and what we'll end up doing today is, uh, fill a straight line to this data, right? And go through how to do that. So in supervised learning, um, the process of supervised learning is that you have a training set, uh, such as the dataset that I drew on the left, and you feed this to learning algorithm, right? And the job of the learning algorithm is to output a function to make predictions about housing prices. And by convention, um, I'm gonna call this function that it outputs a hypothesis. And the job of the hypothesis is, you know, it will, uh, it can input the size of a new house, the size of a different house that you haven't seen yet, and will output the estimated, Price, okay? Um, so the job of the learning algorithm is to input a training set and output a hypothesis. The job of the hypothesis is to take as input any size of a house and try to tell you what it thinks should be the price of that house. Now, when designing a learning algorithm, um, and, and you know, even though, uh, linear regression, right, you may have seen it in a linear algebra class before, in some of the class before. Um, the way you go about structuring the machine learning algorithm is important and design choices of, you know, what is the workflow, what is the dataset, what is the hypothesis, how to represent the hypothesis. These are the key decisions you have to make in pretty much every supervised learning, every machine learning algorithm design. So, uh, as we go through linear regression, I'll try to describe the concepts clearly as well because they'll lay the foundation for the rest of the algorithms, sometimes much more complicated algorithms you see later this quarter. So when designing a learning algorithm, the first thing we'll need to ask is, um, how how do you represent the hypothesis H right And in linear regression the purpose of this lecture we going to say that the hypothesis is going to be that right That the input size x and output a number as a, as a linear function of the size x, okay? And then the mathematicians in the room, you say technically this is an affine function. It was a linear function, there's no theta zero. Technically, you know, but in machine learning, sometimes just call this a linear function but technically it's an affine function, doesn't- doesn't matter. Um, so more generally, in- in this example, we have just one input feature x. Uh, more generally, if you have multiple input features, so if you have more data, more information about these houses, such as number of bedrooms. Excuse me. Well, handwriting is not good. That's the word bedrooms. Right. Then I guess, all right. Yeah. Cool. Yeah. My, my, my father-in-law lives a little bit outside Portland. Uh, he's actually really into real estate. So this is actually a real dataset from Portland. Uh, um, so more generally, uh, if you know the size as well as the number of bedrooms in these houses, then you may have two input features where x1 is the size, and X2 is the number of bedrooms. Um, I'm using the pound sign bedrooms to denote number of bedrooms. And you might say that you estimate the size of a house as, um, h of x equals Theta 0 plus Theta 1 X1 plus Theta 2 X2, where X1 is the size of the house and X2 is, is the number of bedrooms, okay? Um, so in order to, so in order to simplify the notation, um, in order to make that notation a little bit more compact, um, I'm also gonna introduce this other notation where, um, we to write the hypothesis as sum from j equals 0 to 2 of theta j xj so this is a summation where for conciseness we define x0 to be equal to 1 Okay? So if you define, uh, if you define x0 to be a dummy feature that always takes on the value of 1, then you can write the hypothesis h of x this way, sum from j equals 0 to 2 or just Theta j xj. So same that equation that you saw to the upper right. And so here, Theta becomes a three-dimensional parameter, Theta 0, Theta 1, Theta 2, this index starting from 0 and the features become a three-dimensional feature vector x0, x1, x2, where x0 is always 1, x1 is the size of the house and x2 is the number of bedrooms of the house. So, um, so introduce a bit more terminology. Theta is called the parameters, um, of the learning algorithm. And the job of the learning algorithm is to choose parameters theta that allows you to make good predictions about your prices of houses, right? Um, and just to lay out some more, uh, notation that we're gonna use throughout this quarter. I'm gonna use a standard that, uh, M will define as the number of training examples. So M is going to be the number of rows, right, in the table above, um, where, you know, each house you have in your training set is one training example. Um, you've already seen me use x to denote the inputs. Um, and often the inputs are called features. Um, you know, I think, I don't know, as- as- as a- as a emerging discipline grows up, right? Notation kind of emerges depending on what different scientists use for the first time when they write a paper. So I think that, I don't know, I think that the fact that we call these things hypotheses, frankly, I don't think that's a great name, But- but I think someone many decades ago wrote a few papers calling it hypothesis and others follow it and we're kind of stuck with some of this terminology. But X is what's called input features or sometimes input attributes. Um, and Y is the output. Right. And sometimes we call this the target variable. And so x comma y is one training example. And I'm going to use this notation, um, x superstrip i comma y superstrip i in parentheses to denote the i-th training example. Okay. So the superstrip parentheses i, that's not exponentiation. I think that as you build, as it is, this is, um, this notation x i comma y i, this is just a way of writing an index into the table of training examples above. So, so maybe for example, if the first training example is the size, house of size 2104, so x11 would be equal to 2104, right? Because this is the size of the first house in the training example. And I guess x, um, second example, feature 1 would be 1416 with our example above. So the super strip in parentheses is just, um, uh, uh, is- is- is just the, um, index into the different training examples where I, superscript I here would run from 1 through m, 1 through the number of training examples you have. Um, and then one last bit of notation, um, I'm going to use n to denote the number of features you have for the supervised learning problem. So in this example, uh, n is equal to 2, right? uh, because we have two features which is, um, the size of the house and the number of bedrooms. So two features which is why you can take this, right, and write this, um, as a sum from J equals 0 to n. Um, and so here, x and Theta are n plus 1 dimensional because we added the extra, um, x zero and theta zero, okay? So so if you have two features then these are three vectors And more generally if you have n features you end up with x and theta being n plus one dimensional features All right. And, you know, you see this notation multiple times in multiple algorithms throughout this quarter. So if you, you know, don't manage to memorize all these symbols right now, don't worry about it. You see them over and over and they'll become familiar. All right. So, um, Given the dataset and given that this is the way you define the hypothesis, how do you choose the parameters? So, the learning algorithm's job is to choose values for parameters theta so that it can output a hypothesis. So, how do you choose parameters theta? Well, what we'll do is let's choose theta. such that h of x is close to y, uh, for the training examples. So, um, and I think the final bit of notation, um, I've been writing h of x as a function of the features of the house, as a function of the size and number of bedrooms of the house. Um, sometimes they emphasize that h depends both on the parameters theta and on the input features x. I'm going to use H subscript theta of x to emphasize that the hypothesis depends both on the parameters and on the, you know, input features x, right? But sometimes for notational convenience, I just write this as H of x, sometimes I include the theta there and they mean the same thing. It's just maybe an abbreviation in notation, right? But so in order to, um, learn a set of parameters, what we'll want to do is choose a parameter theta, so that at least for the houses whose prices you know, that, you know, the learning algorithm outputs prices that are close to what you know were the correct prices for that set of houses, right? Were the correct asking prices for those houses. And so more formally, um, in the linear regression algorithm, also called ordinary least squares, the linear regression, um, we will want to minimize I gonna build out this equation one piece at a time okay Minimize the squared difference between what the hypothesis outputs H subscript Theta of X minus Y squared right So let's say we want to minimize the squared difference between the prediction which is H of X and Y, which is the correct price. Um, and so what we want to do is choose values of Theta that minimizes that. Um, to fill this out, you know, you have M training examples. So I'm going to sum from i equals 1 through M of that squared difference. So this is sum over i equals 1 through all, say 50 examples you have, right? Um, of the squared difference between what your algorithm predicts and what the true price of the house is. Um, and then finally, by convention we put a one-half there, is to put a one-half constant there because, uh, when we take derivatives to minimize this later, putting one-half there will make some of the math a little bit simpler. So, you know, changing one, adding a one-half. Minimizing that formula should give you the same answer as minimizing one-half of that, but we often put a one-half there since it'll make the math a little bit simpler later. Okay? And so, in, linear regression gonna define the cost function j of theta to be equal to that, and, uh, we'll find parameters theta that minimizes the cost function j of theta. Okay? Um, and questions I've often gotten is, you know, why squared error? Why not absolute error or this error to the power of four? We'll talk more about that when we talk about, um, uh, when, when we talk about the generalization of, uh, linear regression. Um, when we talk about generalized linear models, which we'll do next week, you'll see that, um, uh, linear regression is a special case of a bigger family of arrows called generalized linear models and that, uh, using squared error corresponds to a Gaussian but, but, but, but we'll, we'll, we'll justify maybe a little bit more y squared error rather than absolute error or error to the power of 4 next week. So let me just check see if any questions at this point OK cool All right. So, um, so let's, next let's see how you can implement an algorithm to find the value of Theta that minimizes J of Theta, that minimizes the cost function J of Theta. We're going to use an algorithm called gradient descent. And um, sorry. You know, this is my first class teaching this classroom. So trying to figure out logistics like this. All right. Let's get rid of the chair. Okay, cool. Um, all right. And so with, uh, gradient descent, we are going to start with some value of Theta. Um, and it could be, you know, Theta equals the vector of all zeros, would be a reasonable default. We could initialize it randomly, it kind of doesn't really matter. But theta is this three-dimensional vector, and I'm writing zero with an arrow on top to denote the vector of all zeros. So zero with an arrow on top, there's a vector that says zero, zero, zero everywhere, right? So, um, so start to some, you know, initial value of theta, and we're going to keep changing theta, um, to reduce J of Theta, okay? So, let me show you a, um, but the- but let me show you a visualization of gradient descent, and then- and then we'll write out the math. Um, so, all right. Let's say you want to minimize some function j of theta, and it's important to get the axes right in this diagram, right? So in this diagram, the horizontal axes are theta 0 and theta 1, and what you want to do is find values for theta 0 and theta 1. In our example, it's actually theta 1, 0, Theta 1, Theta 2 because Theta is three-dimensional, but I can't plot that. So I'm just using Theta 0 and Theta 1. But what you want to do is find values for Theta 0 and Theta 1, right? That's the, um, uh, right. You want to find values for Theta 0 and Theta 1, that minimizes the height to the surface J of Theta. So maybe this, this looks like a pretty good point, right, or something. Okay. Um, and so in gradient descent, you, you know, start off at some point on this surface, and you do that by initializing Theta 0 and Theta 1 either randomly or to the value of all zeros or something, doesn't- doesn't matter too much. And, um, what you do is, uh, imagine you're standing on this little hill, right? Standing at the point of that little X or that little cross. Um, what you do in gradient descent is, uh, turn on- turn around all 360 degrees and look all around you, and see if you were to take a tiny little step, you know, take a tiny little baby step, in what direction should you take a little step to go downhill as fast as possible? Because you're trying to go downhill, which is go to the lowest possible elevation, go to the lowest possible point of J of Theta. So what your indiscent will do is stand at that point, look around, look all around you and say, well, what direction should I take a little step in to go downhill as quickly as possible because you want to minimize, uh, J of Theta, you wanna minim- reduce the value of J of Theta. You wanna go to the lowest possible elevation of this hill. Um, and so, gradient descent will take that little baby step, right? And then, and then repeat. Uh, now you're a little bit lower on the surface, so you can take- look all around you and say, oh, it looks like that hill, that- that little direction is the steepest direction or the steepest gradient downhill. So you take another little step, take another step, another step, and so on, until, um, uh, until you, until you get to, uh, hopefully, a local optima. Now, one property of gradient descent is that, um, uh, depending on where you initialize parameters, you can get to local diff- different points, right? So previously, we had started it at that little point x, but imagine if, uh, you had started it, you know, just a few steps over to the right, right? at that new x instead of the one on the left. If you had run grand descent from that new point then that wouldn have been the first step that wouldn have been the second step and so on and you would have gotten to a different local optimum to a different local minimum It turns out that when you run gradient descent on linear regression, it turns out that there will not be local optimum, but we'll talk about that in a little bit. descent, okay? So let's formalize the gradient descent algorithm. In gradient descent, each Each step of gradient descent is implemented as follows. So remember, in this example, the training set is fixed, right? You've collected the data set of housing prices from Portland, Oregon, so you just have that stored in your computer memory. And so the data set is fixed. The cost function J is a fixed function, this function of parameters theta, and the only thing you're going to do is tweak or modify the parameters theta. step of gradient descent can be implemented as follows, which is Theta j gets updated as Theta j minus, I'll just write this out. Okay. So bit more notation, I'm gonna use colon equals, I'm gonna use this notation to denote assignment. So what this means is we're gonna take the value on the right and assign it to Theta on the left, right? And so, um, so in other words, in the notation we'll use this quarter, you know, A colon equals A plus 1. This means increment the value of A by 1. Um, whereas, you know, A equals B. If I write A equals B, I'm asserting a statement of fact, right? I'm asserting that the value of A is equal to the value of B. Um, and hopefully I won't ever write A equals A plus 1, right? because- because that is rarely true, okay? Um, all right. So, uh, in each step of gradient descent, you're going to- for each value of j, so you're gonna do this for j equals 0, 1, 2, or 0, 1 up to n, where n is the number of features. For each value of j take Theta j and update it according to Theta j minus Alpha which is called the learning rate Alpha the learning rate times this formula. And this formula is the partial derivative of the cost function j of Theta with respect to the parameter Theta j, okay? And then this partial derivative of the notation. For those of you that haven't seen calculus for a while or haven't seen, you know, some of their prerequisites for a while, we'll go over some more of this in a little bit greater detail in discussion section, but I'll do this quickly now. But if you take a calculus class a while back, you may remember that the derivative of a function is, defines the direction of steepest descent. So it defines the direction that allows you to go downhill as steeply as possible on the hill like that. Question? How do you determine the learning rate? Oh, how do you determine the learning rate? Let me get back to that. It's a good question. For now, there's a theory and there's a practice. In practice, you set to 0.01. Let me say a bit more about that later. Um, if- if you actually- if- if you scale all the features between 0 and 1, you know, minus 1 and plus 1 or something like that, then- then- yeah, then- then try- you can try a few values and see what lets you minimize the function best, but if the features are scaled to plus minus 1, I usually start with 0.01 and then- and then try increasing and decreasing it. Say- say a little bit more about that. Um, uh, all right, cool. So, um, let's see. Let me just quickly show how the derivative calculation is done. And, you know, I'm, I'm gonna do a few more equations in this lecture, uh, and then, and then over time I think, um, all of this, all of these definitions and derivations are written out in full detail in the lecture notes, uh, posted on the course website. So sometimes I'll do more math in class when, um, we want you to see the steps of the derivation and sometimes to save time in class, we'll gloss over the mathematical details and leave you the read over the full details in the lecture notes on the CS229 course website So partial derivative with respect to J of Theta that the partial derivative with respect to that of um, 1 half h of Theta of x minus y squared. Uh, and so I'm gonna do a slightly simpler version, assuming we have just one training example, right? The, the actual derivate definition of J of Theta has a sum over i from, uh, 1 to m over all the training examples. So I'm just forgetting that sum for now. So if you have only one training example. Um, and so from calculus, if you take the derivative of a square, you know, the 2 comes down and so that cancels out with the half. So 2 times 1 half times, um, uh, the thing inside, right? Uh, and then by the, uh, chain rule of, uh, derivatives, uh, that's times the partial derivative of Theta J of X Theta of X minus Y, right? So if you take the derivative of a square, the two comes down and then you take the derivative of what's inside and multiply that, right? Um, and so the two and the one half cancel out. So this leaves you with H minus Y times partial derivative respect to Theta J of theta 0 x 0 plus theta 1 x 1 plus theta dot plus theta n x n minus y, right? Uh, where I just took the definition of h of x and expanded it out to that, um, to that sum, right? Because, uh, h of x is just equal to that. So if you look at the partial derivative of each of these terms with respect to theta j, the partial derivative of every one of these terms with respect to Theta J is going to be 0 except for the term corresponding to J, right? Because, uh, you know, if- if- if J was equal to 1, say, right, then this term doesn't depend on Theta 1. This term, this term, all of them do not even depend on Theta 1. The only term that depends on Theta 1 is this term over there, Um, and the partial derivative of this term with respect to theta 1 would be just x1, right? And so, um, when you take the partial derivative of this big sum with respect to theta j, uh, in, in, instead of just j equals 1, with respect to theta 1, j in general, then the only term that even depends on theta j is the term theta j xj. And so the partial derivative of all the other terms end up being 0, and partial derivative of this term with respect to theta j is equal to xj, okay? And so this ends up being h theta x minus y times xj, okay? Um, and again, listen, if you haven't, if you haven't played with calculus for a while, if you, you know, don't quite remember what the partial derivative is, or don't quite get what I just said, don't worry too much about it, go over a bit more in the section and we, and then also read through the lecture notes which kind of goes over this in, in, in, um, in more detail and more slowly than, than, uh, we might do in class. Okay. So, um, so plugging this, let's see. So we've just calculated that this partial derivative, right, is equal to this and so plugging it back into that formula, one step of gradient descent is, um, is the following, which is that we will let Theta j be updated according to Theta J minus the learning rate times H of X minus Y times XJ. Okay? Now, um, I'm gonna just add a few more things to this equation. Um, so I did this for one training example, but, uh, this was- I kind of used definition of the cost function J of Theta, defined using just one single training example. but you actually have m training examples. And so, um, the- the- the correct formula for the derivative is actually, if you take this thing and sum it over all m training examples, um, the derivative of- you know, the- the derivative of the sum is the sum of the derivatives, right? So, um, so you actually- if- if- if you redo this derivation, you know, summing with the correct definition of j of Theta, which sums of all m training examples, if you just redo that low derivation, you end up with sum equals i through m of that right Where remember x i is the i training examples input features y i is the target label is the price in the i-th training example, okay? And so this is the actual correct formula for the partial derivative respect to that of the cost function j of theta, when it's defined using, um, uh, all of the, um, uh, when it's defined using all of the training examples, okay? And so the gradient descent algorithm is to repeat until convergence, carry out this update, and in each iteration of gradient descent, uh, you do this update for j equals, uh, 0, 1, up to n, uh, where n is the number of features. So n was 2 in our example. Okay? Um, and if you do this then, uh, uh, you know, actually let me just see. Then what will happen is, um, I'll show you the animation. is you fit, hopefully you find a pretty good value of the parameters theta. So, um, it turns out that when you plot the cost function J of theta for a linear regression model, um, it turns out that unlike the earlier diagram I had shown, which has local optima, it turns out that if J of theta is defined the way that, you know, we just defined it for linear regression, is the sum of squared terms, um, then J of Theta turns out to be a quadratic function, right? It's the sum of these squares of terms. And so J of Theta will always look like, look like a big bowl like this, okay? Um, another way to look at this, and so, and so J of Theta does not have local optima, or the only local optima is also the global optima. The other way to look at the function like this is to look at the contours of this plot, right? So you plot the contours by looking at a big bow and taking horizontal slices and plotting where the, where the curves, where, where the edges of the horizontal slices. So the contours of a big bow I guess the formal is a of a bigger of this quadratic function will be ellipses like these or these ovals or these ellipses like this And so if you run gradient descent on this algorithm, let's say I initialize my parameters at that little x shown over here, right? And usually you initialize theta to the rule of zero, but, but, you know, but it doesn't matter too much. So let's initialize over there. Then, um, with one step of gradient descent, the algorithm will take that step downhill. Uh, and then with the second step, it'll take that step downhill. Oh, and by the way, fun fact, uh, if you- if you think about the contours of the function, it turns out that the direction of steepest descent is always at 90 degrees, is always orthogonal, uh, to the contour direction, right? So, I don't know, yeah. I seem to remember that from my high school or something. I think it's true. All right. And so as you take steps downhill, because there's only one global minimum, this algorithm will eventually converge to the global minimum. Okay. And so the question just now about the choice of the learning rate alpha. If you set alpha to be very, very large, to be too large, then it can overshoot, right? The steps you take can be too large and you can run past the minimum. if you set it to be too small then you need a lot of iterations and the error will be slow. And so what happens in practice is usually you try a few values and, and, and see what value of the learning rate allows you to most efficiently you know drive down the value of J of Theta, right? And if you see J of Theta increasing rather than decreasing, you see the cost function increasing rather than decreasing, then there's a very strong sign that the learning rate is too large. And so, um, actually what- what I often do is actually try out multiple values of, um, the learning rate alpha, and, uh, and- and- and usually try them on exponential scale. So try 0.01, 0.02, 0.04, 0.08, kind of like a doubling scale or some, uh, uh, uh, doubling scale or tripling scale, and try a few values and see what value allows you to drive down the learning rate fastest. Okay. Um, let me just- so I just want to visualize this in one other way which is with the data. So, uh, this is- this is the actual dataset. There are um there are actually 49 points in this dataset So m the number of training examples is 49 And so if you initialize the parameters to zero that means initializing your hypothesis or initializing a straight line fit to the data to be that horizontal line, right? So if you initialize theta zero equals zero, theta one equals zero, then your hypothesis is, you know, for any input size of house over price, the estimated price is zero, right? And so your hypothesis starts off with a horizontal line. That is, whatever the input x, the output y is zero. And what you're doing as you run gradient descent is you're changing the parameters theta, right? So the parameters went from this value to this value to this value to this value and so on. And so the other way of visualizing gradient descent is, if gradient descent starts off with this hypothesis, with each iteration of gradient descent, you are trying to find different values of the parameters data, that allows the straight line to fit the data better. So after one iteration of gradient descent, this is the new hypothesis. You now have different values of theta 0 and theta 1, that fits the data a little bit better. After two iterations, you end up with that hypothesis. And with each iteration of gradient descent, it's trying to minimize J of theta, it's trying to minimize one half of the sum of squares errors of the hypothesis of predictions on the different examples. Right? With three iterations of grand descent, um, uh, four iterations and so on, and then- and then a bunch more iterations, uh, and eventually it converges to that hypothesis, which is pretty- pretty decent straight line fit to the data. Okay. So, question? Go for it. I think I'm visualizing the gradient wrong, but I don't understand why we subtract off the derivative of the cost function times positive constant alpha if the gradient points in the direction of its descent and you want to follow it down that descent? Why is it negative and why is it not the positive? Uh, sure. Maybe, uh, just repeat the question. Why is the, why are you subtracting alpha times the gradient rather than adding alpha times the gradient? Um, let me suggest, actually, let me raise the screen. Um, so let me suggest you work through one example. Um, uh, it turns out that if you add a multiple times a gradient, you'll be going uphill rather than going downhill. And maybe one way to see that would be if, you know, take a quadratic function, excuse me, Right. If you're here, the gradient is a positive direction and you want to reduce. So this would be Theta and this would be J, I guess. So you want Theta to decrease. So the gradient is positive, you want to decrease Theta, so you want to subtract it multiple times the gradient. I think maybe the best way to see that would be to work through an example yourself. Set J of Theta equals Theta squared and set Theta equals one. So here at the quadratic function, the derivative is equal to one. So you want to subtract the value from Theta rather than add. Um, all right. Great. So, you've now seen your first learning algorithm. Um, and, you know, grain descent and linear regression is definitely still one of the most widely used learning algorithms in the world today. And if you implement this, if you, if you, if you implement this today, right, you could use this for, for some actually pretty, pretty decent purposes, right? Um, now, I wanna give this algorithm one other name. Uh, so our gradient descent algorithm here, um, calculates this derivative by summing over your entire training set M. And so, sometimes this version of gradient descent has another name, which is Bash Gradient Descent. Oops. And the term batch, you know, and again, I think in machine learning, a whole community we just make up names of stuff, and sometimes the names aren't great. But the term batch-drained descent refers to that, you look at the entire training set, all 49 examples in the example I just had on PowerPoint. You know, you think of all 49 examples as one batch of data, and we're going to process all the data as a batch. So hence the name, Batch Gradient Descent. The disadvantage of Batch Gradient Descent is that if you have a giant dataset, if you have, um, and, and in the era of big data, we're really moving to larger and larger datasets, right? So I've used, you know, train machine learning models of like hundreds of millions of examples, uh, uh, and, and if you are trying to, if you have a, if you download the US census database, your data on the United States census, that a very large dataset and you want to predict causing prices from all across the United States that may have a data set with many many millions of examples And the disadvantage of batch gradient descent is that in order to make one update to your parameters, in order to take even a single step of gradient descent, you need to calculate this sum, and if M is, say, a million, or 10 million, or 100 million, you need to scan through your entire database, scan for your entire dataset and calculate this for, you know, 100 million examples and sum it up. And so every single step of gradient descent becomes very slow because you're scanning over, you're reading over, right, like 100 million training examples, uh, uh, and, uh, before you can even, you know, make one tiny little step of gradient descent, okay? Um, yeah. By the way, I think, I don't know, I- I- I feel like, uh, in today's era of big data, people start to lose intuitions about what's a big dataset. I think even by today's standards, like a 100 million examples is still very big, right? I, I rarely, only rarely use 100 million examples. Um, I don't know. May, maybe in a few years we'll, we'll look back on 100 million examples and say that was really small, but at least today, uh, yeah. So the main disadvantage of, um, batch re-intercenders, every single step of re-intercend requires that you read through, you know, your entire dataset, maybe terabytes of datasets, maybe, maybe, maybe tens of hundreds of terabytes of data, uh, before you can even update the parameters just once. And if gradient descent needs, you know, hundreds of iterations to converge, then you'll be scanning through your entire data set hundreds of times, right? Or, or, or, and, and sometimes we train our algorithms with thousands of tens of thousands of iterations and so, so this, this gets expensive. So there's an alternative to bash gradient descent. And let me just write out the algorithm here, then we can talk about it, which is going to repeatedly do this. Okay So this algorithm which is called Stochastic Gradient Descent Instead of scanning through all million examples before you update the parameters, state that even a little bit, in Stochastic Gradient Descent, instead in the inner loop of the algorithm, you loop through J equals 1 through M of taking a gradient descent step using the derivative of just one single example, of just that, uh, one example. Oh, excuse me, I, right. So let I go from 1 to M and update Theta j, uh, for every j. So you update this for j equals 1 through n, uh, update Theta j using this derivative that- where now this derivative is taken just with respect to one training example, example i. Okay? Um, I'll just, I guess you update this for every j. Okay? And so let me just draw a picture of what this algorithm is doing. If, um, this is the contour, like the one you saw just now. So the axes are, uh, theta zero and theta one and the height to the surface, right? You know, the contour is this J of theta. With stochastic gradient descent, what you do is you initialize the parameter somewhere, and then, um, you will look at your first training example. Hey, let's just look at one house and see if we can predict that houses better, and you modify the parameters to increase the accuracy where you should predict the price of that one house. And because you're fitting data just to one house, um, you know, maybe you end up improving the parameters a little bit, but not quite going in the most direct direction downhill. And you can look at the second house and say, hey, let's try to fit that house better. And then you update the parameters and you look at third house, fourth house. Right. And so as you run stochastic gradient descent, it takes a slightly noisy, slightly random path, uh, but on average is headed toward the global minimum. Okay. Um, so as you run stochastic gradient descent, So in a sense, we're actually never quite converged. In with backstreet descent, it kind of went to the global minimum and stopped right But stochastic descent even as you want run it the parameters oscillate and won ever quite converge because you always running around looking at different houses and trying to do better on just that one house on that one house, on that one house. Uh, but when you have a very large dataset, um, stochastic gradient descent allows your implementation, allows your algorithm to make much faster progress. Uh, and so, um, uh, uh, and- and so when you have very large datasets, stochastic gradient descent is used much more in practice than batch gradient descent. . Yeah. Oh, is it possible to start with stochastic gradient descent and then switch over to batch gradient descent? Yes, it is. Um, so, uh, boy. Something wasn't a top one in this class, software on CS230 is meaning batch gradient descent where, um, you don't- when you use say a 100 examples at a time rather than one example at a time, uh, and, uh, so that's another algorithm that's actually used more often in practice. I think people rarely- actually, so- so in practice, um, you know, when your dataset is large, um, we rarely ever switch to batch gradient descent, uh, because batch gradient descent is just so slow, right? So I- I don't know, I'm thinking through concrete examples of problems I've worked on. And I think that what, I mean actually, I think that for a lot of, for modern machine learning, we have, if you have very, very large datasets, right? So, you know, whatever, if you're building a speech recognition system, you might have like a terabyte of data, right? And so, it's so expensive to scan through a terabyte of data, just reading it from disk, right? is so expensive that you would probably never even run one iteration of Batch Grand Descent. And it turns out the- the- the- the- the- one- one huge saving grace of Stoke-Constant Grand Descent is, um, let's say you run Stoke-Constant Grand Descent, right? And, you know, you end up with this parameter, and- and that's the parameter you use for your machine learning system rather than the global optimum. It turns out that parameter is actually not that bad, right? you probably make perfectly fine predictions, even if you don't quite get to the global, global minimum. So what you said, I think is a fine thing to do, no harm trying it, although in practice, In practice, we don't bother. I think in practice, we usually use the Sarkozy-Brandeson. The thing that actually is more common is to slowly decrease the learning rate. So just keep using Sarkozy-Brandeson, but reduce the learning rate over time. So it takes smaller and smaller steps. So if you do that, then what happens is the size of the oscillations will decrease. And so you end up oscillating or bouncing around the smaller regions. So wherever you end up may not be the global, global minimum but at least it will be closer to the global minimum. Yes, so decreasing the learning rate is used much more often. Cool. Question? Yeah. . Oh, sure. When do you stop the ? Plot to J of Theta over time. So J of Theta is a cost function that you're trying to drive down. So monitor J of Theta as it's going down over time, and then if it looks like it stopped going down, then you can say, oh, it looks like it's stopped going down, I'm gonna stop training. Uh, although- and then, um, you know, one nice thing about linear regression is, uh, it has no local optimum and so, um, uh, it- you run into these convergence debugging types of issues less often. When you're training highly non-linear things like neural networks, which we'll talk about later in CS229 as well, uh, these issues become more acute. All right, cool. Okay, great. So, um, oh, yeah. . Oh, which one of your learning rate would be one of the rent times learning rate for batch in descent? Not really. It's usually much bigger than that. Yeah. Yeah. Because if your learning rate was one of rent times that of what you use as batch in descent then it end up being as slow as batch in descent. So, it's usually much bigger. Okay. So, um, so that's stochastic gradient descent. Oh, and, and so I, I'll tell you what I do. If, if you have a relatively small dataset, you know, if you have, if you have, I don't know, like hundreds of examples, maybe thousands of examples where, uh, it's computationally efficient to do batch gradient descent. If batch gradient descent doesn't cost too much, I would almost always just use batch gradient descent because it's one less thing to fiddle with, right? It's just one less thing to have to worry about, uh, the parameters oscillating. But if your dataset is too large that batch gradient descent becomes pro prohibitively slow then uh almost everyone would use you know stochastic gradient descent instead right Or however more like stochastic gradient descent Okay All right So, um, gradient descent, both Batch-Grain Descent and Sarkozy-Grain Descent is an iterative algorithm, meaning that you have to take multiple steps to get to, you know, get near hopefully the global optimum. It turns out this is another algorithm, oh, and, and, um, for many other algorithms we'll talk about in this class, including generalized linear models and neural networks and a few other algorithms, you will have to use gradient descent. And so, and so we'll see gradient descent, you know, as we develop multiple different algorithms later this quarter. It turns out that for the special case of linear regression, and I mean linear regression but not the algorithm we'll talk about next Monday, not the algorithm we'll talk about next Wednesday, but if the algorithm you're using is linear regression and exactly linear regression, it turns out that there's a way to solve for the optimal value of the parameters theta to just jump in one step to the global optimum, uh, without needing to use an iterative algorithm, right? And- and this- this- this one I'm gonna present next is called the normal equation. It works only for linear regression, doesn't work for any of the other algorithms we'll talk about later this quarter. But, um, uh, let me quickly show you the derivation of that. And, um, what I want to do is, uh, give you a flavor of how to derive the normal equation. And where you end up with is, you know, what- what- what I hope to do is end up with a formula that lets you say, theta equals some stuff where you just set theta equals to that, and in one step with a few matrix multiplications, you end up with the optimal value of theta that lands you right at the global optimum, right? kind of just like that, just in one step, okay? Um, and if you've taken, you know, advanced linear algebra classes before or something, you may have seen, um, this formula for linear regression. Uh what what what a lot of linear algebra classes do is um what some linear algebra classes do is cover the board with you know pages and pages of matrix derivatives What I want to do is describe to you a matrix derivative notation that allows you to derive the normal equation in roughly four lines of linear algebra rather than sort of pages and pages of linear algebra. And in the work I've done in machine learning, you know, sometimes notation really matters, right? If you write notation you can solve some problems much more easily. And what I want to do is define this matrix linear algebra notation and then I don't want to do all the steps of the derivation, I want to give you a, give you a sense of the flavor of what it looks like and then I'll ask you to get a lot of details yourself in the, in the lecture notes where I work out everything in more detail than I want to do algebra in class. Oh and in problem set one, you get to practice using this yourself to, to, to, you know, derive some additional things. I've, I've found this notation really convenient, right, uh, for deriving learning algorithms. Okay. So, um, I'm going to use the following notation. Um, so j, right, was the function mapping from parameters to the real numbers. So I'm going to define this, this is the derivative of J of Theta with respect to Theta. Where remember Theta is a three-dimensional vector, so it's R3, or actually it's R n plus 1, right? If you have, uh, two features of the hulls, if n equals 2, then Theta is three-dimensional, it's n plus one-dimensional, so it's a vector. And so I'm gonna define the derivative with respect to theta of J of theta as follows. Um, this is going to be itself a three by one vector. Okay. So, I hope this notation is clear. So this is a three-dimensional vector with three components. Wow. So that's, boy, get some... So that the first component of this vector that the second and that the third okay It a partial derivative of J respect to each of the three elements Yeah Um and more generally in the notation we'll use, um, let me give you an example. Um, let's say that A is a matrix. So let's say that's a- A is a two by two matrix. Then, um, you can have a function, right? So let's say A is, you know, A11, A12, A21, A22, right? So A is a two by two matrix. Then you might have some function, um, of a matrix A, right, then that's a real number. So maybe F maps from A 2 by 2 to, oh, excuse me, R 2 by 2 to a real number. So, um, uh, and so for example, if F of A equals A 1 1 plus A 1 2 squared, then F of, you know, 5, 6, 7, 8 would be equal to I guess 5 plus 6 squared, right? So as we derive this, we'll be working a little bit with functions that map from matrices to real numbers. And this is just one made up example of a function that inputs a matrix and maps the matrix, maps the values of the matrix to real number. And when you have a matrix function like this, I'm going to define the derivative with respect to A of f of A, to be equal to itself a matrix where the derivative of f of a with respect to the matrix A, uh, this itself will be a matrix with the same dimension of A and the elements of this are the derivative with respect to the individual elements. Okay. So if A was a two by two matrix, then the derivative of f of A respect to A is itself a two by two matrix, and you compute this two by two matrix just by looking at F and taking derivatives with respect to the different elements and plugging them into the different elements of this matrix. Okay. And so in this particular example, I guess the derivative respect to A of f of A, this would be, um, right? It would be, it would be that. Uh, and I got these four numbers by taking, um, the definition of f and taking the derivative with respect to a11 and plugging that here. Right. Uh, taking the derivative with respect to a12 and plugging that here, and taking the derivative with respect to the remaining elements and plugging them here, which, which, which was 0, okay? So that's the definition of a matrix derivative. Yeah? . Oh, yes. Would you use the same definition for a vector, uh, n by 1 or n by 1 matrix? Yes. And in fact, um, that definition and this definition for the, the, the derivative of j with respect to theta, these are consistent. So if you apply that definition to a column vector, treating a column vector as an n by 1 matrix or n plus n, I guess it'd be n plus 1 by 1 matrix, then that specializes to what we describe here. All right. So, um, Let's see. Okay. So, um, I want to leave the details of the lecture notes because there is more lines of algebra I'm gonna want to do, but I'll give you an overview of what the derivation of the normal equation looks like. Um, so arms of this definition of a derivative of a, of a matrix, uh, the- the broad outline of what we're going to do is, we're going to take J of Theta, right? That the cost function Um take the derivative with respect to theta right Uh since theta is a vector so we want to take the derivative with respect to Theta And, you know, well, how do you minimize the function? You take the derivative with respect to Theta and set it equal to 0, and then you solve for the value of Theta so that the derivative is 0, right? The minimum, you know, the maximum or minimum function is where the derivative is equal to 0. So, so how you derive the normal equation is take this vector, So J of Theta maps from a vector to a real number. So we'll take derivatives with respect to Theta, set the derivative to go to 0 and solve for Theta. And then we'll end up with a formula for Theta that lets you just, um, uh, you know, immediately go to the global minimum of the, of the cost function J of Theta. And, and, and a lot of the build up, a lot of this notation is, you know, is there- what does this mean and is there an easy way to compute the derivative of J of Theta? Okay. So, um, uh, to help you understand the lecture notes when hopefully you take a look at them, uh, just a couple other derivations. Um, if A is a square matrix, so let's say A is, uh, an n by n matrix, so number of rows equals number of columns, um, I'm going to denote the trace of A, to be equal to the sum of the diagonal entries. So, sum of i of a ii. And this is pronounced the trace of a. And, and, and you could, you could also write this as trace operator, like the trace function applied to a, but by convention we often write trace of a without the parentheses. And so this is called the trace of A. So trace just means sum of diagonal entries. And, um, some facts about the trace of a matrix, you know, trace of A is equal to the trace of A transpose, because if you transpose a matrix, right, you're just flipping it along the, the 45 degree axis and so the, the diagonal entries actually stay the same when you transpose the matrix. So the trace of A is equal to trace of A transpose. Um, and then, uh, there, there are some other useful properties of, um the trace operator Um here one that I don want to prove but that you could go home and prove yourself with a with a few with some with a little bit of work maybe not not too much Which is, uh, if you define, um, f of A equals trace of A times B. So here, if B is some fixed matrix, right? And what f of A does is it multiplies A and B, and then it takes the sum of diagonal entries, then it turns out that the derivative with respect to A of f of A is equal to, um, B transpose. Um, and this is, uh, you could prove this yourself. For any matrix B, if f of A is defined this way, the derivative is equal to B transpose. Um, the trace function or the trace operator has other interesting properties. The trace of AB is equal to the trace of BA. Um, um, you could, you could prove this from first principles. A little bit of work to prove, uh, uh, that, that you, if you expand out the definition of A and B, you should prove that. And the trace of A times B times C is equal to, the trace of C times A times B. Uh, this is a, a cyclic permutation property. If you have a multiple, you know, multiply several matrices together, you can always take one from the end and move it to the front, and the trace will remain the same, right? And, um, another one that is a little bit harder to prove is that the trace, excuse me, derivative of A trans- of A A transpose C is, okay? Yeah. So I think just as- just as for, you know, ordinary calculus we know the derivative of x squared is 2x, right? And so we all figured out that rule and we just use it too much without, without having to re-derive it every time. This is a little bit like that. The trace of a squared c is, you know, 2 times cA, right? It's a little bit like that, but, but with, with matrix notation as that. So think of this as analogous to d dA of a squared c equals 2ac, right? But it like the matrix version of that All right So finally, um, what I'd like to do is take J of theta and express it in this, uh, you know, matrix vector notation. So we can take the rotors with respect to theta and set the rotors equal to 0 and just solve for the value of theta, right? And so, um, let me just write out the definition of J of theta. So J of Theta was one-half sum from i equals 1 through m of, uh, h of x i minus y i squared. Um, and it turns out that, um, all right. It turns out that, um, if you des- if you define a matrix capital X as follows, which is I'm going to take the matrix capital X and, uh, take the training examples we have, you know, and stack them up in rows. So we have M training examples, right? So- so the Xs were column vectors. So I'm taking columns, right? to just stack up the training examples in, uh, in rows here. So I'm gonna call this the design matrix, uh, the Kappa X called the design matrix. And, um, uh, it turns out that if you define X this way, then X times Theta is this thing times Theta. And, uh, the way matrix vector multiplication works is, you know, Theta is now a column vector, right? So Theta is, you know, Theta 0, Theta 1, Theta 2. So the way that, um, matrix vector multiplication works is you multiply this column vector with each of these in, in turn. And so this ends up being X1 transpose Theta, X2 transpose Theta down to Xm. Transposed Theta, which is of course just the vector of all of the predictions of the algorithm. And so if, um, now let me also define a vector y to be taking all of the, um, labels from your training example and stacking them up into a big column vector, right? Let me define y that way. Um, it turns out that, um, J of Theta can then be written as one-half of X Theta minus Y transpose X Theta minus Y. Okay? Um, and let me see. Yeah. Let me just, uh, uh, outline the proof but I won't do this in great detail. So x Theta minus y is going to be, right? So this is x Theta, this is y. So, you know, x Theta minus y is going to be this vector of h of x1 minus y1, down to h of xm minus ym, right? So it's just all the errors you're learning out from this making on the m examples. the difference between predictions and the actual labels. And, um, if you- you- you remember, so Z transpose Z is equal to sum over I Z squared right A vector transpose itself is the sum of squares of elements And so this vector transpose itself is the sum of squares of the elements which is y So the cost function j of theta is computed by taking the sum of squares of all of these elements, of all of these errors. And the way you do that is to take this vector, x theta minus y transpose itself, is the sum of squares of these, which is exactly the error. So that's why you end up with this is the sum of squares of the, those error terms. And if some of the steps don't quite make sense, really don't worry about it, all this is written out more slowly and carefully in the lecture notes, but I wanted you to have a sense of the broad arc of the big picture of the derivation before you go through them yourself in greater detail in the lecture elsewhere. So finally, what we want to do is take the derivative respect to theta of j of theta and set that to zero. And so this is going to be equal to the derivative of one half x Theta minus y transpose x Theta minus y. Um, and so I'm gonna, I'm gonna do the steps really quickly, right? So the steps require some of the little properties of traces and matrix derivatives that I wrote down briefly just now. But so I'm gonna do these very quickly without going into the details. But, uh, so this is equal to one-half derivatives with Theta of, um, so take transposes of these things. So this becomes Theta transpose X transpose minus Y transpose. Okay. Um, and then, uh, kind of like expanding out a quadratic function right This is you know A minus B times C minus d so you can just ac minus ad and so on So I just write this out And so what I just did here, this is similar to how, you know, Ax minus b times Ax minus b is equal to a squared, x squared minus Ax b minus bAx plus b squared. It's kind of, it's just expanding out the quadratic function. Um, and then the final step is, yeah, go ahead. . Uh, is that right? Oh yes, thank you. Great, thank you. Um, and then the final step is, you know, for each of these four terms, first, second, third, and fourth terms, to take the derivative with respect to Theta. And if you use some of the formulas I was alluding to over there, you find that the derivative, um, which- which I don't want to show the derivation of, But it turns out that the derivative is, um, X transpose X Theta plus X transpose X Theta minus, um, X transpose Y minus X transpose Y. Um, and we're going to set this derivative. Actually, let me just do this. And so this simplifies to X transpose X Theta minus X transpose Y. And so as described earlier I gonna set this derivative to zero And how to go from this step to that step is using the matrix derivatives explained in more detail in the lecture notes And so, the final step is, you know, having set this to zero, this implies that X transpose X theta equals X transpose Y. Um, so this is called the normal equations. And, uh, the optimum value for Theta is Theta equals X transpose X inverse, X transpose Y, okay? Um, and if you implement this, um, then, you know, you can in basically one step get the value of Theta that corresponds to the global minimum. Okay. Um, and, and, and again common question I get at this point is, well what if X is non-invertible? What that usually means is you have redundant features, that your features are linearly dependent. But if you use something called the pseudo-inverse, you, you kind of get the right answer if that's the case. Although I think the even more right answer is if you have linearly dependent features probably means you have the same feature repeated twice and I would usually go and figure out what features actually repeated leading to this problem. Okay. All right. Any last questions before? So that, so that's the normal equations. Hope you read through the detailed derivations in the lecture notes. Any last questions for Ray? Yeah, how do you choose the learning rate? It's quite empirical I think. So most people try different values and then just pick one. All right, I think let's break. If people have more questions, when the TAs come up, we can keep taking questions. But let's break for today. Thanks everyone.